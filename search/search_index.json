{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Welcome to VisualFlow Wiki!","text":"<p>Visual Flow is a low-code ETL/ELT solution with an open source license that has Apache Spark, Kubernetes, and Argo Workflows under the hood of a drag-and-drop interface.</p>"},{"location":"#contents","title":"Contents:","text":"<ol> <li>About Visual Flow</li> <li>Getting started with Visual Flow:     2.1 AWS     2.2 Azure     2.3 Google     2.4 Minikube</li> <li>Visual Flow services:     3.1 Backend     3.2 Backend db service     3.3 Backend history service     3.4 Frontend     3.5 Jobs(Spark)</li> </ol>"},{"location":"#our-web-site","title":"Our web site","text":"<p>Visit our shiny web-site: https://visual-flow.com</p>"},{"location":"#contact-us","title":"Contact us!","text":"<p>Feel free to contacts us: info@visual-flow.com</p>"},{"location":"About-Visual-Flow/","title":"General info","text":"<p>Visual Flow is an ETL/ELT tool designed for effective data management via convenient and user-friendly interface. The tool has the following capabilities:</p> <ul> <li>Can integrate data from heterogeneous sources:</li> <li>Azure Blob Storage</li> <li>AWS S3</li> <li>Cassandra</li> <li>Click House</li> <li>DB2</li> <li>Databricks JDBC (global configuration)</li> <li>Databricks (Databricks configuration)</li> <li>Dataframe (for reading)</li> <li>Google Cloud Storage</li> <li>Elastic Search</li> <li>IBM COS</li> <li>Kafka</li> <li>Local File</li> <li>MS SQL</li> <li>Mongo</li> <li>MySQL/Maria</li> <li>Oracle</li> <li>PostgreSQL</li> <li>Redis</li> <li>Redshift</li> <li>REST API</li> <li>It supports the following file formats:</li> <li>Delta Lake</li> <li>Parquet</li> <li>JSON</li> <li>CSV</li> <li>ORC</li> <li>Avro</li> <li>Text</li> <li>Binary (PDF, DOC, Audio files)</li> <li>Leverage direct connectivity to enterprise applications as sources and targets</li> <li>Perform data processing and transformation</li> <li>Run custom code</li> <li>Leverage metadata for analysis and maintenance</li> <li>Allows to deploy in two configurations and run jobs in Spark/Kubernetes and Databricks environments respectively</li> <li>Leverages Generative AI capabilities via tasks like Parse text, Generate data, Transcribe, Generic task</li> </ul> <p>Visual Flow application is divided into the following repositories:</p> <ul> <li>Visual-Flow-frontend</li> <li>Visual-Flow-backend</li> <li>Visual-Flow-jobs</li> <li>Visual-Flow-deploy</li> <li>Visual-Flow-backend-db-service</li> <li>Visual-Flow-backend-history-service</li> </ul>"},{"location":"About-Visual-Flow/#contribution","title":"Contribution","text":"<p>Check the official guide.</p>"},{"location":"About-Visual-Flow/#license","title":"License","text":"<p>Visual flow is an open-source software licensed under the Apache-2.0 license.</p>"},{"location":"About-Visual-Flow/CONTRIBUTING/","title":"How to contribute to Visual Flow","text":"<p>First off, thanks for taking the time to contribute!</p>"},{"location":"About-Visual-Flow/CONTRIBUTING/#did-you-find-a-bug","title":"Did you find a bug?","text":"<ul> <li>Ensure the bug was not already reported by searching on GitHub under Issues.</li> <li>If you're unable to find an open issue addressing the problem, open a new one. Be sure to include a title and clear description, as much relevant information as possible, and a code sample or an executable test case demonstrating the expected behavior that is not occurring. For more detailed information on submitting a bug report and creating an issue, visit our reporting guidelines.</li> </ul>"},{"location":"About-Visual-Flow/CONTRIBUTING/#did-you-write-a-patch-that-fixes-a-bug","title":"Did you write a patch that fixes a bug?","text":"<ul> <li>Open a new GitHub pull request with the patch. Development team will be reviewing pull requests on regular basis and according to team\u2019s priorities and bandwidth.</li> <li>Ensure the PR description clearly describes the problem and solution. Include the relevant issue number if applicable.</li> </ul>"},{"location":"About-Visual-Flow/CONTRIBUTING/#did-you-fix-whitespace-format-code-or-make-a-purely-cosmetic-patch","title":"Did you fix whitespace, format code, or make a purely cosmetic patch?","text":"<p>Changes that are cosmetic in nature and do not add anything substantial to the stability, functionality, or testability of Visual Flow will not be accepted.</p>"},{"location":"About-Visual-Flow/CONTRIBUTING/#do-you-intend-to-add-a-new-feature-or-change-an-existing-one","title":"Do you intend to add a new feature or change an existing one?","text":"<ul> <li>Contact us via email: info@visual-flow.com</li> <li>Do not open an issue on GitHub until you have collected positive feedback about the change. GitHub issues are primarily intended for bug reports and fixes.</li> </ul>"},{"location":"About-Visual-Flow/CONTRIBUTING/#do-you-have-questions-about-the-source-code","title":"Do you have questions about the source code?","text":"<p>Ask any question about Visual Flow via email: info@visual-flow.com</p> <p>Visual Flow team</p>"},{"location":"About-Visual-Flow/Changelog/","title":"Changelog","text":""},{"location":"About-Visual-Flow/Changelog/#150-2025-03-11","title":"1.5.0 (2025-03-11)","text":""},{"location":"About-Visual-Flow/Changelog/#improvements-and-new-features","title":"Improvements and new features:","text":"<ul> <li>Added GenAI feature (new stage).</li> <li>Added Incremental load feature (Read stage).</li> <li>Added Interactive mode feature (new 'Debugging' status) with the ability to preview metadata and data exemplar.</li> <li>Added SQL editor with syntax markup.</li> <li>Added new connectors (Databricks, Azure Blob Storage, Google Cloud Storage, Kafka).</li> <li>Added new types, formats and parcers (Delta, Binary, PDF, DOC/DOCX etc).</li> <li>Added Sandbox mode.</li> <li>Updated Spark version to 3.4.1.</li> <li>Added hints for stage helper.</li> <li>Changed search crieria for Users/Roles to search by the name instead of id.</li> <li>Improved CDC stage: processing null values.</li> <li>Added ability to copy/paste stages between jobs within the same project (namespace).</li> <li>Changed boolean drop down lists to a new toggle element.</li> <li>Improved security to manage connection passwords.</li> <li>Improved search placeholders.</li> <li>Improved Avro schema.</li> <li>Improved UI: prevention to run empty jobs and pipelines.</li> <li>Improved import/export functionality.</li> <li>Added new sort option 'By last edit' for Jobs and Pipelines.</li> <li>Improved sorting method.</li> <li>Added possibility to sort jobs/pipelines in Pipeline Designer.</li> <li>Cosmetic updates in JD for drop down lists, scroll bars and multiline fields. \u200b</li> </ul>"},{"location":"About-Visual-Flow/Changelog/#fixed","title":"Fixed:","text":"<ul> <li>Date/time stage issue.</li> <li>Role-based issues.</li> <li>Jobs/Pipelines dependency issue for 'Copy' action.</li> <li>Notification parameters issue.</li> <li>Fields duplication issue.</li> <li>Drop-down list moves while scrolling the page issue.</li> <li>Logs level list moves while choosing a level.</li> <li>Incorrect rows number in resulting STDOUT for write stage issue.</li> <li>Incorrect data in Dataframe modal window.</li> <li>Typo issues in different modes.</li> <li>Jobs vulnerabilities (CVE-2022-33891, CVE-2021-25642, CVE-2021-33036, CVE-2021-37404, CVE-2020-36632, CVE-2020-10650, CVE-2020-9480, CVE-2008-1997, CVE-2021-32626, CVE-2021-40531, CVE-2018-7489, CVE-2020-35491, CVE-2020-35490, CVE-2020-10673, CVE-2017-7525, CVE-2022-25168, CVE-2023-25194, CVE-2019-14887, CVE-2023-22946, CVE-2020-8840).</li> <li>DBService vulnerabilities (CVE-2024-45772, CVE-2024-1597, CVE-2024-7254, CVE-2024-32888, CVE-2024-22262, CVE-2024-38809, CVE-2024-22243, CVE-2024-38816). \u200b \u200b</li> </ul>"},{"location":"About-Visual-Flow/Changelog/#141-2023-09-04","title":"1.4.1 (2023-09-04)","text":""},{"location":"About-Visual-Flow/Changelog/#fixed_1","title":"Fixed:","text":"<ul> <li>Error with export/import that creates invalid configurations.</li> <li>Error with stage name duplication.</li> <li>Vulnerabilities (CVE-2022-42889, CVE-2023-26136, CVE-2023-28154).</li> </ul>"},{"location":"About-Visual-Flow/Changelog/#140-2023-07-04","title":"1.4.0 (2023-07-04)","text":""},{"location":"About-Visual-Flow/Changelog/#improvements-and-new-features_1","title":"Improvements and new features:","text":"<ul> <li>Updated Spark version to 3.3.2.</li> <li>Added API data source.</li> <li>Updated Add/Update Column stage with regex functionality.</li> <li>Updated Add/Update Column stage to adjust precision and scale for decimal type.</li> <li>Updated GroupBy stage to allow several aggregations for one field via one operator.</li> <li>Added option to join dataframes with different key names.</li> <li>Detailed info is returned about the error from DB.</li> <li>Projects list is limited depending on the user access.</li> <li>Cosmetic updates in JD for config panel.</li> <li>Improved Job configuration to read huge configurations from file as an option. \u200b</li> </ul>"},{"location":"About-Visual-Flow/Changelog/#fixed_2","title":"Fixed:","text":"<ul> <li>JD/PD does not allow saving stages with the same name.</li> <li>Scheduling notifications/changes.</li> <li>Job status set to 'Failed' once Notification is failed due to problems with creds for Slack or email.</li> <li>Tags do not disappear after moving to another tab.</li> <li>Errors are displayed in the appropriate logging level.</li> <li>Typo issues in JD. \u200b \u200b</li> </ul>"},{"location":"About-Visual-Flow/Changelog/#130-2023-04-03","title":"1.3.0 (2023-04-03)","text":""},{"location":"About-Visual-Flow/Changelog/#major-release-notes","title":"Major Release Notes:","text":"<p>For existing projects only. Once the deployment process of updates for 1.3.0 is done it's required to run Release_1_3_0_workflow_fix.sh script to avoid configuration errors in existing projects.  \u200b</p>"},{"location":"About-Visual-Flow/Changelog/#improvements-and-new-features_2","title":"Improvements and new features:","text":"<ul> <li>Implemented notification service as part of Pipeline template - email and Slack.</li> <li>Added auto refresh option for Job Designer page.</li> <li>Changed Validate stage configuration.</li> <li>Log button is hidden for a stage if it hasn't been run yet.</li> <li>Updated context menu for copy/paste stage.</li> <li>New label for withColumn stage -&gt; Add/Update Column.</li> <li>Updated new project resources.</li> <li>Improved import/export functionalities for Jobs/Pipelines.</li> <li>Allowed export Jobs which are in Pipelines.</li> <li>Added limits while entering Job's/Pipeline's name.</li> <li>Improved dependencies functionality.</li> <li>Updated Scheduling with UTC/user's time zones.</li> <li>Cosmetic updates in JD/PD and Parameter/Connection config panel. \u200b</li> </ul>"},{"location":"About-Visual-Flow/Changelog/#fixed_3","title":"Fixed:","text":"<ul> <li>Fixed tooltips for CDC stage.</li> <li>Ping after changing connection parameters.</li> <li>Fixed error handling during login with private GitHub account.</li> <li>Log out.</li> <li>Long Job/Pipeline name is truncated in import modal window.</li> <li>Job can't be saved because of negative values in resources.</li> <li>Fixed 'Logs' in Pipeline Jobs.</li> <li>Fixed message error in JD params panel. \u200b \u200b</li> </ul>"},{"location":"About-Visual-Flow/Changelog/#120-2023-02-21","title":"1.2.0 (2023-02-21)","text":""},{"location":"About-Visual-Flow/Changelog/#improvements-and-new-features_3","title":"Improvements and new features:","text":"<ul> <li>Updated design of parameters panel.</li> <li>Added Pipeline stage to the PD.</li> <li>Implemented 'Logs' section for Pipeline history panel.</li> <li>Adapted Job/Pipeline Logs to show new added levels automatically.</li> <li>Added suspend/resume Pipeline functionality.</li> <li>Removed Pipeline's status Stopped from UI.</li> <li>Made PD read-only for the Suspended status.</li> <li>Current Pipeline is hidden from the Pipelines list.</li> <li>Implemented ClickHouse as a new source.</li> <li>Pipeline isn't removed if one exists in any Pipeline.</li> <li>Parameters/Connections aren't removed if they are used in any Jobs or Pipelines.</li> <li>New stages in the Job Designer - Pivot, String Functions, Data/Timestamp, WithColumn.</li> <li>Added mode tag to Slice stage when configured.</li> <li>Orange header with suggestion to save is shown in a case of move/rearrangement of stages on canvas.</li> <li>Added app version to the user profile menu.</li> <li>Implemented avatars for GitHub / GitLab auth types.</li> <li>Added language switcher &amp; support multi languages.</li> <li>Updated colors at the MUI scheme level with cosmetic updates of Project Overview page. \u200b</li> </ul>"},{"location":"About-Visual-Flow/Changelog/#fixed_4","title":"Fixed:","text":"<ul> <li>Fixed description for Transformer stage.</li> <li>Fixed border color for Wait stage.</li> <li>Connection isn't saved if don't enter a value in required field.</li> <li>Drop-down list opens correctly in PD.</li> <li>Read/Write Stage - corrected order in Storage dropdown.</li> <li>Corrected texts style on Configuration panel for all stages.</li> <li>Sticked action buttons to the bottom for PD.</li> <li>Fixed console errors.</li> <li>The 'Save' button is inactive if there are no changes in PD/JD.</li> <li>Reformated/optimized check Job usages in any other Pipelines method. \u200b \u200b</li> </ul>"},{"location":"About-Visual-Flow/Changelog/#110-2022-11-11","title":"1.1.0 (2022-11-11)","text":""},{"location":"About-Visual-Flow/Changelog/#improvements-and-new-features_4","title":"Improvements and new features:","text":"<ul> <li>Updating the application design.</li> <li>New storages - ClickHouse, Dataframe, Local File.</li> <li>New stage in Pipeline Designer - Wait. </li> <li>Implemented 'Ping' functionality for connections.</li> <li>Updated amount of required fields for Configuration panel.</li> <li>Set 'Name' field as required on the Connection Configuration panel. </li> <li>The 'Error' notification is displayed for 5 seconds only.</li> <li>The 'Run' button is disabled while editing params for Job and Pipeline Designers.</li> <li>After opening in the Parameters modal window, a selected parameter will be displayed.</li> <li>Changed cursor to pointer for Profile logo.</li> <li>Changed functionality on the Connections page. \u200b</li> </ul>"},{"location":"About-Visual-Flow/Changelog/#fixed_5","title":"Fixed:","text":"<ul> <li>Fixed the orange header, which disappears after clicking 'Confirm' button on the Configuration panel.</li> <li>Hide 'Local File' and 'STDOUT' storages from Connections.</li> <li>Fixed table display in the Users window.</li> <li>Connection is saved if a user doesn't enter a value in the required field.</li> <li>Added 'CertData' field in the Connections Configuration panel.</li> <li>Fixed drop-down list while page is scrolling.</li> <li>Fixed the 'White screen' issue.</li> <li>The status in Job history is updated depending on the current status. \u200b \u200b</li> </ul>"},{"location":"About-Visual-Flow/Changelog/#100-2022-10-10","title":"1.0.0 (2022-10-10)","text":""},{"location":"About-Visual-Flow/Changelog/#improvements-and-new-features_5","title":"Improvements and new features:","text":"<ul> <li>Updating the application design.</li> <li>Added Tags for Pipelines and Jobs.</li> <li>Implemented Connections window to specify credentials for database.</li> <li>Added hint line to Scheduling window.</li> <li>Implemented History of Job running.</li> <li>Removed 'Key' field for Cross type in Join stage.</li> <li>New stage in the Job Designer - Slice.</li> <li>Implemented the display of avatar. </li> <li>Redesigned the Job's modal window for the Pipeline Designer page.</li> <li>Reduced scale to 20 on the Job Designer page.</li> <li>Added Notifications configuration panel to the Job Designer.</li> <li>There is a possibility to copy stage on the canvas.</li> <li>Added hints for stage in the Job Designer.</li> <li>Job History button is displayed on the panel of Job Designer.</li> <li>Show Confirmation window, when user closes Job or Pipeline Designer page if there's any change.</li> <li>Cosmetic updates of different pages. </li> <li>Implemented Search function via tags.</li> <li>Added the ability to open a Job via Pipeline.</li> <li>On Connections panel added 'Database' field for Mongo and Redshift storages and CertData field for Cassandra storage. \u200b</li> </ul>"},{"location":"About-Visual-Flow/Changelog/#fixed_6","title":"Fixed:","text":"<ul> <li>Fixed the 'Cancel' button in the Schema window.</li> <li>Fixed the 'Last Run' filter for the application used in Firefox browser.</li> <li>Shows local time instead of UTC for humanized dates.</li> <li>Jobs and Pipelines lists keep sort after Job or Pipeline running, link sharing, Job or Pipeline Designer opening and page updating.</li> <li>Fixed issue with 'Scheduling' icon on the Pipelines list.</li> <li>Pipeline 'Running' status is displayed via Scheduling.</li> <li>Fixed 'Clear' icon near Delimiter field.</li> <li>Connection fields are cleared if the connection is changed to a connection with the same fields but empty.</li> <li>Fixed incorrect display of fields in the Connection window.</li> <li>Pipeline running is disabled after removing Job inside it.</li> <li>Fixed incorrect links order for Join stage.</li> <li>Fixed incorrect display of the Truncate mode label.</li> <li>The same parameter values are not considered as duplicates for Parameters window.</li> <li>In the parameters window, field don't move up, when a hint appears.</li> <li>Fixed the stop of Pipeline running via Scheduling.</li> <li>Fixed Params functionality in the Job Designer. \u200b \u200b</li> </ul>"},{"location":"About-Visual-Flow/Changelog/#0919-2022-07-14","title":"0.9.19 (2022-07-14)","text":""},{"location":"About-Visual-Flow/Changelog/#improvements-and-new-features_6","title":"Improvements and new features:","text":"<ul> <li>Changed behavior of project level permissions: once user first logins into application, he sees all projects as locked.</li> <li>Implemented a new file format for IBM COS and AWS S3 storages - 'Avro'.</li> <li>Added 'Avro' type description to the Job Designer</li> <li>Removed 'Eye' icon near 'Parameters' icon in the Job Designer Configuration panel. Instead, added 'Eye' icon to Parameters window for secure string.</li> <li>Created Read and Write schema for file format 'Avro' for IBM COS and AWS S3 storages.</li> <li>Implemented functionality for disabling schema.</li> <li>Humanized dates on the UI.</li> <li>Changed the location of fields in the Configuration panel for Mongo storage.</li> <li>Created 'Cron' for Pipelines to run at the appointed time.</li> <li>New possibility to choose the number of Jobs or Pipelines per page. </li> <li>Implemented the SSL field for AWS S3 connection. \u200b</li> </ul>"},{"location":"About-Visual-Flow/Changelog/#fixed_7","title":"Fixed:","text":"<ul> <li>Fixed displaying 'Logs' icon and 'Warning' inside the stage in the Pipeline Designer.</li> <li>Fixed the 'Undo' function.</li> <li>An error message appears if a user has entered an invalid Image pull secret name.</li> <li>Fixed bug that appeared when a user copies some filters in the Job Designer.</li> </ul>"},{"location":"About-Visual-Flow/Changelog/#this-document-contains-release-versions-with-the-release-dates-in-the-following-format","title":"This document contains release versions with the release dates in the following format:","text":""},{"location":"About-Visual-Flow/Changelog/#agreement-on-release-numbering","title":"Agreement on release numbering","text":"<p>Release number contains three digits separated by point (major_version.minor_version.bug_fix), where: - major release version - new UI, lots of features, conceptual changes etc. - minor release version - several changes, a set of bug fixes - bug fix release</p>"},{"location":"About-Visual-Flow/Changelog/#agreement-on-the-release-date-format","title":"Agreement on the release date format","text":"<ul> <li>The date format is yyyy-mm-dd</li> </ul> <p>So, the general release representation is the following: Release_number (yyyy-mm-dd)</p>"},{"location":"Getting-started-with-Visual-Flow/","title":"General info","text":"<p>Visual Flow is an ETL/ELT tool designed for effective data management via convenient and user-friendly interface. The tool has the following capabilities:</p> <ul> <li>Can integrate data from heterogeneous sources:</li> <li>Azure Blob Storage</li> <li>AWS S3</li> <li>Cassandra</li> <li>Click House</li> <li>DB2</li> <li>Databricks JDBC (global configuration)</li> <li>Databricks (Databricks configuration)</li> <li>Dataframe (for reading)</li> <li>Google Cloud Storage</li> <li>Elastic Search</li> <li>IBM COS</li> <li>Kafka</li> <li>Local File</li> <li>MS SQL</li> <li>Mongo</li> <li>MySQL/Maria</li> <li>Oracle</li> <li>PostgreSQL</li> <li>Redis</li> <li>Redshift</li> <li>REST API</li> <li>It supports the following file formats:</li> <li>Delta Lake</li> <li>Parquet</li> <li>JSON</li> <li>CSV</li> <li>ORC</li> <li>Avro</li> <li>Text</li> <li>Binary (PDF, DOC, Audio files)</li> <li>Leverage direct connectivity to enterprise applications as sources and targets</li> <li>Perform data processing and transformation</li> <li>Run custom code</li> <li>Leverage metadata for analysis and maintenance</li> <li>Allows to deploy in two configurations and run jobs in Spark/Kubernetes and Databricks environments respectively</li> <li>Leverages Generative AI capabilities via tasks like Parse text, Generate data, Transcribe, Generic task</li> </ul> <p>Visual Flow application is divided into the following repositories: </p> <ul> <li>Visual-Flow-frontend</li> <li>Visual-Flow-backend</li> <li>Visual-Flow-jobs</li> <li>Visual-Flow-deploy (current)</li> <li>Visual-Flow-backend-db-service</li> <li>Visual-Flow-backend-history-service</li> </ul>"},{"location":"Getting-started-with-Visual-Flow/#visual-flow-deploy","title":"Visual Flow deploy","text":"<p>This repository contains helm chart to deploy Visual Flow app with all requirements to Amazon EKS cluster.</p> <p>Helm charts in this repository:</p> <ul> <li>visual-flow - to deploy Visual Flow application.</li> <li>dbs - to deploy Redis/PostgreSQL databases.</li> </ul>"},{"location":"Getting-started-with-Visual-Flow/#installation","title":"Installation","text":"<p>Check the official guide.</p>"},{"location":"Getting-started-with-Visual-Flow/#contribution","title":"Contribution","text":"<p>Check the official guide.</p>"},{"location":"Getting-started-with-Visual-Flow/#license","title":"License","text":"<p>Visual Flow is an open-source software licensed under the Apache-2.0 license.</p>"},{"location":"Getting-started-with-Visual-Flow/SLACK_NOTIFICATION/","title":"Connect Visual Flow notifications to a Slack Workspace","text":"<p>In order to allow Visual Flow to send Slack notifications from a pipeline, Visual Flow needs to be added as a bot user to your Slack Workspace.</p> <p>Below are the steps for adding a bot user to your Slack Workspace.</p> <ol> <li> <p>Open the following link.</p> </li> <li> <p>In the <code>'Create an app'</code> window select <code>'From scratch'</code>.</p> </li> <li> <p>In the <code>'Name app &amp; choose workspace'</code> window, populate the <code>'App name'</code> field (for example with 'Visual Flow') and in the <code>'Pick a workspace to develop your app in'</code> field select your workspace. Then click the <code>'Create App'</code> button.</p> </li> <li> <p>Choose the <code>OAuth &amp; Permissions</code> tab on the left sidebar.</p> </li> <li> <p>Below the <code>Bot Token Scopes</code> click on <code>'Add an OAuth Scope'</code> and select the following scopes:</p> <p><code>yaml users:read (View people in a workspace) users:read.email (View email addresses of people in a workspace) chat:write (Send messages as Visual flow) chat:write.public (Send messages to channels Visual flow isn't a member of) channels:read (View basic information about public channels in a workspace) groups:read (View basic information about private channels that Visual flow has been added to) im:read (View basic information about direct messages that Visual flow has been added to) mpim:read (View basic information about group direct messages that Visual flow has been added to)</code></p> </li> <li> <p>Above in the <code>'OAuth Tokens for Your Workspace'</code> section click on <code>'Install to Workspace'</code> then click the <code>'Allow'</code> button.</p> </li> <li> <p>Copy the generated <code>'Bot User OAuth Token'</code> and paste it to the <code>SLACK_API_TOKEN</code> variable in values.yaml in the downloaded repository.</p> </li> <li> <p>Return to the installation guide from which you were redirected to this doc and continue installing.</p> </li> </ol> <p>More information about 'Add a bot user' can be found here.</p>"},{"location":"Getting-started-with-Visual-Flow/AWS/AWS/","title":"Installation Visual Flow to Amazon Elastic Kubernetes Service (EKS)","text":""},{"location":"Getting-started-with-Visual-Flow/AWS/AWS/#prerequisites","title":"Prerequisites","text":"<p>To install Visual Flow you should have the following software already installed:</p> <ul> <li>AWS CLI (install)</li> <li>kubectl (install)</li> <li>eksctl (install)</li> <li>Helm CLI (install)</li> <li>Git (install)</li> </ul> <p>IMPORTANT: all the actions are recommended to be performed from the admin/root AWS account.</p> <p>If you have just installed the AWS CLI, then you need to log in using following command:</p> <p><code>aws configure</code></p>"},{"location":"Getting-started-with-Visual-Flow/AWS/AWS/#create-an-eks-cluster","title":"Create an EKS cluster","text":"<p>Visual Flow should be installed on an EKS cluster. For the full functionality - recomended use regular AWS EKS claster (if you need Backend &amp; UI). In case if you need just backend and limited functions of frontend UI (without db &amp; history services) - you can use and Fargate claster.</p> <p>You can create cluster using following commands:</p>"},{"location":"Getting-started-with-Visual-Flow/AWS/AWS/#eks-fargate","title":"EKS Fargate:","text":"<pre><code>export CLUSTER_NAME=visual-flow\n\neksctl create cluster \\\n--fargate \\\n--name $CLUSTER_NAME \\\n--region us-east-1 \\\n--with-oidc \\\n--full-ecr-access \\\n--external-dns-access \\\n--alb-ingress-access\n\n# duration: ~20min\n# if creation failed delete cluster using following command and repeat from beginning\n# eksctl delete cluster --region us-east-1 --name $CLUSTER_NAME\n\n# check access\nkubectl get nodes\nkubectl get pods --all-namespaces\n</code></pre>"},{"location":"Getting-started-with-Visual-Flow/AWS/AWS/#eks-regular-cluster-ec2-instance-type-m5large-with-one-node","title":"EKS Regular cluster (EC2 instance type 'm5.large' with one Node)","text":"<p>IMPORTANT: You should have at least 1 large node (8GB memory and 2 vCPU) to be able to run VF application on your cluster.</p> <pre><code>export CLUSTER_NAME=visual-flow\n\neksctl create cluster \\\n--name $CLUSTER_NAME \\\n--region us-east-1 \\\n--with-oidc \\\n--ssh-access \\\n--full-ecr-access \\\n--external-dns-access \\\n--alb-ingress-access \\\n--instance-types=m5.large \\\n--managed \\\n--nodes 1\n\n# duration: ~30min\n# if creation failed delete cluster using following command and repeat from beginning\n# eksctl delete cluster --region us-east-1 --name $CLUSTER_NAME\n\n# check access\nkubectl get nodes\nkubectl get pods --all-namespaces\n</code></pre> <p>For additional info check following guide:</p> <p>https://docs.aws.amazon.com/eks/latest/userguide/getting-started-eksctl.html</p>"},{"location":"Getting-started-with-Visual-Flow/AWS/AWS/#connect-to-existing-eks-cluster-from-local-machine","title":"Connect to existing EKS cluster from local machine","text":"<p>If you have an EKS cluster, you can connect to it using the following command:</p> <p><code>aws eks --region &lt;REGION&gt; update-kubeconfig --name &lt;CLUSTER_NAME&gt;</code></p>"},{"location":"Getting-started-with-Visual-Flow/AWS/AWS/#check-access-to-eks-cluster","title":"Check access to EKS cluster","text":"<p>Run the following command to check access to the EKS cluster from the local machine:</p> <p><code>kubectl get nodes</code></p> <p>If you get the message \"<code>error: You must be logged in to the server (Unauthorized)</code>\", you can try to fix it using the following guide:</p> <p>https://aws.amazon.com/premiumsupport/knowledge-center/eks-api-server-unauthorized-error/</p> <p>If you have access to the EKS cluster on a different computer, you can on another computer try to provide access to the cluster for the local machine using the following guide:</p> <p>https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html</p> <p>For more information on how to use EKS on fargate check the following guide:</p> <p>https://docs.aws.amazon.com/eks/latest/userguide/fargate-getting-started.html</p>"},{"location":"Getting-started-with-Visual-Flow/AWS/AWS/#install-an-aws-load-balancer-alb-to-eks","title":"Install an AWS Load Balancer (ALB) to EKS","text":"<p>AWS Load Balancer allows you to access applications on EKS from the Internet by hostname. If you don't have it installed, then install it. You can install ALB using following commands:</p> <pre><code># add ALB policy\ncurl -o iam_policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/v2.8.1/docs/install/iam_policy.json\n# the following command will fail if the policy already exists\naws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam_policy.json\n\n# create SA for ALB\nexport ACCOUNT_ID=&lt;ACCOUNT_ID&gt;\neksctl create iamserviceaccount --cluster=$CLUSTER_NAME --namespace=kube-system --name=aws-load-balancer-controller --attach-policy-arn=arn:aws:iam::${ACCOUNT_ID}:policy/AWSLoadBalancerControllerIAMPolicy --override-existing-serviceaccounts --approve\n\n# install ALB via helm chart\nhelm repo add eks https://aws.github.io/eks-charts || helm repo update\n\n# set correct VPC ID for ALB\nVPC_ID_TMP=$(aws eks describe-cluster --name $CLUSTER_NAME | grep vpc- | cut -d ':' -f 2)\nVPC_ID=$(echo $VPC_ID_TMP)\n\nhelm upgrade -i aws-load-balancer-controller eks/aws-load-balancer-controller --set clusterName=$CLUSTER_NAME --set region=us-east-1 --set vpcId=$VPC_ID --set serviceAccount.create=false --set serviceAccount.name=aws-load-balancer-controller -n kube-system\n\n# wait until all pods will be ready\nkubectl get pods --all-namespaces\n</code></pre> <p>For additional info check following guide:</p> <p>https://docs.aws.amazon.com/eks/latest/userguide/aws-load-balancer-controller.html</p> <p>https://docs.aws.amazon.com/eks/latest/userguide/network-load-balancing.html</p>"},{"location":"Getting-started-with-Visual-Flow/AWS/AWS/#install-an-efs-controller-to-eks-for-automatic-pvc-provisioning","title":"Install an EFS Controller to EKS (for automatic PVC provisioning)","text":"<p>How to install Amazon EFS CSI driver:</p> <p>https://docs.aws.amazon.com/eks/latest/userguide/efs-csi.html</p> <p>GitHub source of EFS CSI controller:</p> <p>https://github.com/kubernetes-sigs/aws-efs-csi-driver</p> <p>Depend from your choice - you can use or Dynamic provisioning (PV &amp; PVC will be created and mounted to StorageClass automatically):</p> <p>https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/examples/kubernetes/dynamic_provisioning/README.md</p> <p>...or Static provisioning (you will need to create PV by yourself with required config):</p> <p>https://github.com/kubernetes-sigs/aws-efs-csi-driver/blob/master/examples/kubernetes/static_provisioning/README.md</p>"},{"location":"Getting-started-with-Visual-Flow/AWS/AWS/#install-redis-postgresql","title":"Install Redis &amp; PostgreSQL","text":"<p>Some functionality of VF app requires to have Redis &amp; PosgreSQL dbs. Both of them with custom and default configs included in installation as a separate helm charts (values files with source from bitnami repo). </p> <p>https://github.com/ibagroup-eu/Visual-Flow-deploy/tree/amazon/charts/dbs</p> <p>You can get them and install on you cluster using following commands:</p> <p>Add 'bitnami' repository to helm repo list</p> <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\n</code></pre> <ol> <li>Redis (for Session and Job's execution history)</li> </ol> <p><code>helm install redis -f bitnami-redis/values.yaml bitnami/redis</code></p> <ol> <li>PostgreSQL (History service)</li> </ol> <p><code>helm install pgserver -f bitnami-postgresql/values.yaml bitnami/postgresql</code></p> <p>FYI: Just in case better to save output of these command (it contains helpful info with short guide, how to get access to pod &amp; dbs and show default credentials).</p>"},{"location":"Getting-started-with-Visual-Flow/AWS/AWS/#install-visual-flow","title":"Install Visual Flow","text":"<ol> <li> <p>Clone (or download) the Amazon branch from Visual-Flow-deploy repository on your local computer using following command:</p> <p><code>git clone -b amazon https://github.com/ibagroup-eu/Visual-Flow-deploy.git Visual-Flow-deploy</code></p> </li> <li> <p>Go to the directory \"visual-flow\" of the downloaded \"Visual-Flow-Deploy\" repository with the following command:</p> <p><code>cd Visual-Flow-deploy/charts/visual-flow</code></p> </li> <li> <p>(Optional) Configure Slack notifications in values.yaml using following guide:</p> <p>https://github.com/ibagroup-eu/Visual-Flow-deploy/blob/main/SLACK_NOTIFICATION.md</p> </li> <li> <p>Set superusers in values.yaml.</p> <p>New Visual Flow users will have no access in the app. The superusers(admins) need to be configured to manage user access. Specify the superusers real GitHub nicknames in values.yaml in the yaml list format:</p> <p><code>yaml superusers:   - your-github-nickname   # - another-superuser-nickname</code></p> </li> <li> <p>If you have installed kube-metrics then update values.yaml file according to the example below.</p> <ol> <li> <p>Check that the kube-metrics installed using the following command:</p> <p><code>bash kubectl top pods</code></p> <p>Output if the kube-metrics isn't installed:</p> <p><code>error: Metrics API not available</code></p> <p>If the kube-metrics isn't installed then go to step 6.</p> </li> <li> <p>Edit values.yaml file according to the example below:</p> <p><code>yaml ... kube-metrics:   install: false</code></p> </li> </ol> </li> <li> <p>If you have installed Argo workflows then update values.yaml file according to the example below.</p> <ol> <li> <p>Check that the Argo workflows installed using the following command:</p> <p><code>bash kubectl get workflow</code></p> <p>Output if the Argo workflows isn't installed:</p> <p><code>error: the server doesn't have a resource type \"workflow\"</code></p> <p>If the Argo workflows isn't installed then go to step 7.</p> </li> <li> <p>Edit values.yaml file according to the example below:</p> <p><code>yaml ... argo:   install: false vf-app:   backend:     configFile:       argoServerUrl: &lt;Argo-Server-URL&gt;</code></p> </li> </ol> </li> <li> <p>Install the app using the updated values.yaml file with the following command:</p> <p><code>helm upgrade -i vf-app . -f values.yaml</code></p> </li> <li> <p>Check that the app is successfully installed and all pods are running with the following command:</p> <p><code>kubectl get pods --all-namespaces</code></p> </li> <li> <p>Get the generated app's hostname with the following command:</p> <p><code>kubectl get svc vf-app-frontend -o yaml | grep hostname | cut -c 17-</code></p> <p>Replace the string <code>&lt;HOSTNAME_FROM_SERVICE&gt;</code> with the generated hostname in the next steps.</p> </li> <li> <p>Create a GitHub OAuth app:</p> <ol> <li>Go to GitHub user's OAuth apps (<code>https://github.com/settings/developers</code>) or organization's OAuth apps (<code>https://github.com/organizations/&lt;ORG_NAME&gt;/settings/applications</code>).</li> <li>Click the Register a new application or the New OAuth App button.</li> <li>Fill the required fields:<ul> <li>Set Homepage URL to <code>https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/</code></li> <li>Set Authorization callback URL to <code>https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/callback</code></li> </ul> </li> <li>Click the Register application button.</li> <li>Replace \"DUMMY_ID\" with the Client ID value in values.yaml.</li> <li>Click Generate a new client secret and replace in values.yaml \"DUMMY_SECRET\" with the generated Client secret value (Please note that you will not be able to see the full secret value later).</li> </ol> </li> <li> <p>Update 'uiHost' (<code>uiHost: https://&lt;HOSTNAME_FROM_SERVICE&gt;</code>) and 'STRATEGY_CALLBACK_URL' (<code>STRATEGY_CALLBACK_URL: https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/callback</code>) values in values.yaml. </p> </li> <li> <p>Upgrade the app in EKS cluster using updated values.yaml:</p> <p><code>helm upgrade vf-app . -f values.yaml</code></p> </li> <li> <p>Wait until the update is installed and all pods are running:</p> <p><code>kubectl get pods --all-namespaces</code></p> </li> </ol>"},{"location":"Getting-started-with-Visual-Flow/AWS/AWS/#use-visual-flow","title":"Use Visual Flow","text":"<ol> <li> <p>All Visual Flow users (including superusers) need active Github account in order to be authenticated in application. Setup Github profile as per following steps:</p> <ol> <li>Navigate to the account settings</li> <li>Go to Emails tab: set email as public by unchecking Keep my email addresses private checkbox</li> <li>Go to Profile tab: fill in Name and Public email fields</li> </ol> </li> <li> <p>Open the app's web page using the following link:</p> <p><code>https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/</code></p> </li> <li> <p>See the guide on how to work with the Visual Flow at the following link: Visual_Flow_User_Guide.pdf</p> </li> <li> <p>For each project Visual Flow generates a new namespace. For each namespace, you should create a Fargate profile to allow running jobs and pipelines in the corresponding project.</p> </li> </ol> <p>First, create the project in the app, open it and check the URL of the page. It will have the following format:</p> <p><code>https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/&lt;NAMESPACE&gt;/overview</code></p> <p>Get namespace from this URL and use in the following command to create fargate profile:</p> <pre><code>`eksctl create fargateprofile --cluster &lt;CLUSTER_NAME&gt; --region &lt;REGION&gt; --name vf-app --namespace &lt;NAMESPACE&gt;`\n</code></pre>"},{"location":"Getting-started-with-Visual-Flow/AWS/AWS/#delete-visual-flow","title":"Delete Visual Flow","text":"<ol> <li> <p>If the app is no longer required, you can delete it using the following command:</p> <p><code>helm uninstall vf-app</code></p> </li> <li> <p>Check that everything was successfully deleted with the command:</p> <p><code>kubectl get pods --all-namespaces</code></p> </li> </ol>"},{"location":"Getting-started-with-Visual-Flow/AWS/AWS/#delete-additional-components","title":"Delete additional components","text":"<p>If you do no need them anymore - you can also delete and these additional components:</p> <ol> <li>Redis &amp; PostgreSQL databases</li> </ol> <p><code>helm uninstall redis</code></p> <p><code>helm uninstall pgserver</code></p> <ol> <li>EFS &amp; LoadBalancer Controllers</li> </ol> <p><code>helm uninstall aws-efs-csi-driver</code></p> <p><code>helm uninstall aws-load-balancer-controller</code></p>"},{"location":"Getting-started-with-Visual-Flow/AWS/AWS/#delete-eks","title":"Delete EKS","text":"<ol> <li>If the EKS is no longer required, you can delete it using the following guide:</li> </ol> <p><code>eksctl delete cluster --name visual-flow</code></p> <p>https://docs.aws.amazon.com/eks/latest/userguide/delete-cluster.html</p>"},{"location":"Getting-started-with-Visual-Flow/azure/Rook_ceph_on_AKS_guide/","title":"Rook ceph on AKS guide","text":"<p>If you want to understand how to install rook-ceph on AKS and configure it yourself, you can read about it here: https://devpress.csdn.net/k8s/62ebdd3d89d9027116a0fa0d.html</p> <p>If you want to install it just for Visual Flow projects you can follow the guide below:</p> <p>For this install we use: - Rook-ceph v1.12.0 https://github.com/rook/rook/tree/v1.12.0 - We expect that you already have AKS installed and configured including Visual Flow install INSTALL.md - One extra nodepool for AKS to be created manually for rook-ceph (in this guide we call it <code>rookcephfs</code>). - 1 node (Standard_B2ms machine). - We recommend to use at least 2vCPU and 8Gb RAM for this rook-ceph node to be able to start and run it succesfully. If you try with worse configuration this install may fail with unexpected errors\\warnings.</p> <ol> <li>Add Storage Node to your AKS cluster:</li> </ol> <pre><code>az aks nodepool add --cluster-name &lt;YOUR_CLUSTER_NAME&gt; \\\n--name rookecephfs --resource-group &lt;YOUR_RESOURCE_GROUP&gt; \\ \n--node-count 1 \\\n--node-taints storage-node=true:NoSchedule\n</code></pre> <ol> <li>Make sure you can see your new node:</li> </ol> <pre><code>kubectl get nodes\n</code></pre> <ol> <li>Install commons.yaml:</li> </ol> <pre><code>kubectl create -f rook-ceph_1.12.0/common.yaml\n</code></pre> <ol> <li>Create AKS operator operator-aks.yaml:</li> </ol> <pre><code>kubectl create -f rook-ceph_1.12.0/operator-aks.yaml\n</code></pre> <ol> <li>Create AKS CephCluster cluster-aks.yaml. Defaults are: 1 node, 10G storage and rookcephfs nodepool name. If you need to change storage, please take a look at this file and update <code>spec.mon.volumeClaimTemplate.spec.resources.requests.storage</code> and <code>spec.storageClassDeviceSets.volumeClaimTemplates.spec.resources.requests.storage</code>. If you have another nodepool name, please update <code>spec.storageClassDeviceSets.placement.nodeAffinity.requiredDuringSchedulingIgnoredDuringExecution.nodeSelectorTerms.matchExpressions.values</code>.</li> </ol> <pre><code>kubectl create -f rook-ceph_1.12.0/cluster-aks.yaml\n</code></pre> <ol> <li>Wait a couple of minutes and check status using next commands. If CephCluster status is not HEALTH_OK or you do not see rook-ceph-osd-0-<code>CephID</code> pods then you need to verify previous steps to make sure you did not missed and check logs. Status check and expected output:</li> </ol> <pre><code>$ kubectl get -n rook-ceph CephCluster.ceph.rook.io/rook-ceph\nNAME        DATADIRHOSTPATH   MONCOUNT   AGE     PHASE   MESSAGE                        HEALTH      EXTERNAL   FSID\nrook-ceph   /var/lib/rook     1          2m42s   Ready   Cluster created successfully   HEALTH_OK              d6e47925-b86e-4b12-b10d-aea1d39c327e\n\n$ kubectl get pods -n rook-ceph | grep -i rook-ceph-osd\nrook-ceph-osd-0-8844ff8c-slfc5                                    1/1     Running     0          60s\nrook-ceph-osd-prepare-set1-data-0mvx2c-6s6wb                      0/1     Completed   0          82s\n</code></pre> <ol> <li>Once you have HEALTH_OK status and OSD pod running you can create a cephFilesystem and a rook-ceph storageclass:</li> </ol> <pre><code>kubectl create -f rook-ceph_1.12.0/storageclass-fs-aks.yaml\n</code></pre> <ol> <li> <p>Now you need to change your default storage class to be <code>rook-cephfs</code> that we just created. How to change your default storage class: https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/</p> </li> <li> <p>Done! Once you changed your default storageclass all your new Visual Flow project PVCs will be provisioned by rook-ceph. Create a new project using Visual Flow and verify that you have BOUND status of your PVC <code>vf-pvc</code> in Visual Flow project namespace:</p> </li> </ol> <pre><code>$ get pvc -n &lt;YOUR_VF_PROJECT_NAMESPACE&gt;\nNAME     STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE\nvf-pvc   Bound    pvc-65ff9574-513e-4ee2-8053-2476423811f2   2Gi        RWX            rook-cephfs-storage   1d\n</code></pre>"},{"location":"Getting-started-with-Visual-Flow/azure/azure/","title":"Installation Visual Flow to Azure Kubernetes Service (AKS)","text":""},{"location":"Getting-started-with-Visual-Flow/azure/azure/#prerequisites","title":"Prerequisites","text":"<p>IMPORTANT: This installation requires access to our private Container Registry. Please contact us to get access: info@visual-flow.com</p> <p>To install Visual Flow on AKS you should have the following software on your local\\master machine already installed:</p> <ul> <li>Azure CLI (install)</li> <li>kubectl (install)</li> <li>Helm CLI (install)</li> <li>Git (install)</li> </ul> <p>IMPORTANT: all the actions are recommended to be performed from the Microsoft account with owner privileges.</p> <p>If you have just installed the Azure CLI, then you need to log in using following command:</p> <p><code>az login</code></p> <p>And create a Resource Group (if you do not have any) with the location that suits you best:</p> <p><code>az group create --name MyResourceGroup --location centralus</code></p>"},{"location":"Getting-started-with-Visual-Flow/azure/azure/#create-aks-cluster","title":"Create AKS cluster","text":"<p>IMPORTANT: if you are new to AKS, please read about AKS cluster: cluster types,config params and pricing (https://learn.microsoft.com/en-us/azure/aks/)</p> <p>Visual Flow should be installed on AKS cluster. You can create AKS cluster using following commands:</p> <pre><code>export RESOURCE_GROUP=MyResourceGroup\nexport CLUSTER_NAME=visual-flow\nexport LOCATION=centralus\nexport NUM_NODES=2\nexport NUM_ZONES=1\naz aks create --resource-group $RESOURCE_GROUP --name $CLUSTER_NAME --node-count $NUM_NODES --location $LOCATION --zones $NUM_ZONES --generate-ssh-keys\n\n# check access\naz aks get-credentials --resource-group $RESOURCE_GROUP --name $CLUSTER_NAME\nkubectl get pods --all-namespaces\n</code></pre> <p>For additional info check following guide:</p> <p>https://learn.microsoft.com/en-us/azure/aks/learn/quick-kubernetes-deploy-cli</p>"},{"location":"Getting-started-with-Visual-Flow/azure/azure/#connect-to-existing-aks-cluster-from-local-machine","title":"Connect to existing AKS cluster from local machine","text":"<p>If you have AKS cluster, you can connect to it using the following command:</p> <p><code>az aks get-credentials --resource-group &lt;YOUR_RESOURCE_GROUP&gt; --name &lt;YOUR_CLUSTER_NAME&gt;</code></p>"},{"location":"Getting-started-with-Visual-Flow/azure/azure/#install-visual-flow","title":"Install Visual Flow","text":"<ol> <li> <p>Clone (or download) the azure branch from Visual-Flow-deploy repository on your local computer using following command:</p> <p><code>git clone -b azure https://github.com/ibagroup-eu/Visual-Flow-deploy.git Visual-Flow-azure-deploy</code></p> </li> <li> <p>Go to the directory \"visual-flow\" of the downloaded \"Visual-Flow-Deploy\" repository with the following command:</p> <p><code>cd Visual-Flow-azure-deploy/charts/visual-flow</code></p> </li> <li> <p>(Optional) Configure Slack notifications (replace <code>YOUR_SLACK_TOKEN</code>) in values-az.yaml using the following guide:</p> <p>https://github.com/ibagroup-eu/Visual-Flow-deploy/blob/main/SLACK_NOTIFICATION.md</p> </li> <li> <p>Set superusers in values-az.yaml.</p> <p>New Visual Flow users will have no access in the app. The superusers(admins) need to be configured to manage user access. Specify the superusers real GitHub nicknames in values-az.yaml in the yaml list format:</p> <p><code>yaml superusers:   - your-github-nickname   # - another-superuser-nickname</code></p> </li> <li> <p>(Optional) If you want, you can install kube-metrics then update values-az.yaml file according to the example below. </p> <ol> <li> <p>Check if the kube-metrics installed using the following command:</p> <p><code>bash kubectl top pods</code></p> <p>Output if the kube-metrics isn't installed:</p> <p><code>error: Metrics API not available</code></p> <p>If the kube-metrics is already installed then go to step 6.</p> </li> <li> <p>Edit values-az.yaml file according to the example below:</p> <p><code>yaml ... kube-metrics:   install: true</code></p> </li> </ol> </li> <li> <p>If you have installed Argo workflows then update values.yaml file according to the example below.</p> <ol> <li> <p>Check that the Argo workflows installed using the following command:</p> <p><code>bash kubectl get workflow</code></p> <p>Output if the Argo workflows isn't installed:</p> <p><code>error: the server doesn't have a resource type \"workflow\"</code></p> <p>If the Argo workflows isn't installed then go to step 7.</p> </li> <li> <p>Edit values-az.yaml file according to the example below:</p> <p><code>yaml ... argo:   install: false vf-app:   backend:     configFile:       argoServerUrl: &lt;Argo-Server-URL&gt;</code></p> </li> </ol> </li> <li> <p>Install Redis database. If you have installed Redis database then just update values.yaml file according to the example below.</p> <ol> <li> <p>Create a namespace for Redis. In current configuration =redis: <p><code>bash kubectl create namespace &lt;REDIS_NAMESPACE&gt;</code></p> <li> <p>Update Helm repo:</p> <p><code>bash helm repo update</code></p> </li> <li> <p>Install Redis with predefined values.yaml (./charts/dbs/bitnami-redis/values.yaml):</p> <p><code>bash helm install -n &lt;REDIS_NAMESPACE&gt; redis bitnami/redis -f ../dbs/bitnami-redis/values.yaml</code></p> </li> <li> <p>Update <code>redis.host</code> and <code>redis.password</code> of backend and frontend section in values-az.yaml file according to the example below:</p> <p>```yaml ... redis:   host: redis-master..svc.cluster.local    port: 6379"},{"location":"Getting-started-with-Visual-Flow/azure/azure/#username-redis_user","title":"username: ${REDIS_USER}","text":"<p>password: #    database: 1 ...   frontend:     deployment:       variables:         ...         REDIS_HOST: redis-master..svc.cluster.local         ...       secretVariables:         ...         REDIS_PASSWORD: #  ``` <li> <p>Install PostgreSQL database. If you have installed PostgreSQL database then just update values.yaml file according to the example below.</p> <ol> <li> <p>Create a namespace for PostgreSQL. In current configuration =postgres: <p><code>bash kubectl create namespace &lt;PostgreSQL_NAMESPACE&gt;</code></p> <li> <p>Update Helm repo:</p> <p><code>bash helm repo update</code></p> </li> <li> <p>Install PostgreSQL with predefined values.yaml (./charts/dbs/bitnami-postgresql/values.yaml):</p> <p><code>bash helm install -n &lt;PostgreSQL_NAMESPACE&gt; postgresql bitnami/postgresql -f ../dbs/bitnami-postgresql/values.yaml</code></p> </li> <li> <p>Update <code>PG_URL</code>, <code>PG_USER</code> and <code>PG_PASS</code> in values-az.yaml file according to the example below:</p> <p><code>yaml ... historyserv:   configFile:     postgresql:       PG_URL: jdbc:postgresql://postgresql.&lt;PostgreSQL_NAMESPACE&gt;.svc.cluster.local:5432/postgres # postgres_url       PG_USER: # postgres_username       PG_PASS: # postgres_password</code> 9. Prepare namespace for Visual Flow.</p> </li> <li> <p>Create a namespace for Visual Flow. In current configuration =visual-flow: <p><code>bash kubectl create namespace &lt;VF_NAMESPACE&gt;</code></p> <li> <p>(Optional) Set visual-flow namespace to be default in your profile:</p> <p><code>bash kubectl config set-context --current --namespace=&lt;VF_NAMESPACE&gt;</code></p> </li> <li> <p>Install the app using the updated values-az.yaml file with the following command:</p> <p><code>helm install visual-flow . -f values-az.yaml -n &lt;VF_NAMESPACE&gt;</code></p> </li> <li> <p>Check that the app is successfully installed and all pods are running with the following command:</p> <p><code>kubectl get pods -n &lt;VF_NAMESPACE&gt;</code></p> </li> <li> <p>Get the generated app's hostname with the following command:</p> <p><code>kubectl get svc visual-flow-frontend -n &lt;VF_NAMESPACE&gt; -o yaml | grep -i clusterIP: | cut -c 14-</code></p> <p>Replace the string <code>&lt;EXTERNAL_IP_FROM_SERVICE&gt;</code> with the generated hostname in the next steps.</p> </li> <li> <p>Create a GitHub OAuth app:</p> <ol> <li>Go to GitHub user's OAuth apps (<code>https://github.com/settings/developers</code>) or organization's OAuth apps (<code>https://github.com/organizations/&lt;ORG_NAME&gt;/settings/applications</code>).</li> <li>Click the Register a new application or the New OAuth App button.</li> <li>Fill the required fields:<ul> <li>Set Homepage URL to <code>https://&lt;EXTERNAL_IP_FROM_SERVICE&gt;/vf/ui/</code></li> <li>Set Authorization callback URL to <code>https://&lt;EXTERNAL_IP_FROM_SERVICE&gt;/vf/ui/callback</code></li> </ul> </li> <li>Click the Register application button.</li> <li>Replace \"DUMMY_ID\" with the Client ID value in values-az.yaml.</li> <li>Click Generate a new client secret and replace in values-az.yaml \"DUMMY_SECRET\" with the generated Client secret value (Please note that you will not be able to see the full secret value later).</li> </ol> </li> <li> <p>Update STRATEGY_CALLBACK_URL value in values-az.yaml to <code>https://&lt;EXTERNAL_IP_FROM_SERVICE&gt;/vf/ui/callback</code></p> </li> <li> <p>Upgrade the app in EKS cluster using updated values.yaml:</p> <p><code>helm upgrade visual-flow . -f values-az.yaml -n &lt;VF_NAMESPACE&gt;</code></p> </li> <li> <p>Wait until the update is installed and all pods are running:</p> <p><code>kubectl get pods -n &lt;VF_NAMESPACE&gt;</code></p> </li>"},{"location":"Getting-started-with-Visual-Flow/azure/azure/#use-visual-flow","title":"Use Visual Flow","text":"<ol> <li> <p>All Visual Flow users (including superusers) need active Github account in order to be authenticated in application. Setup Github profile as per following steps:</p> <ol> <li>Navigate to the account settings</li> <li>Go to Emails tab: set email as public by unchecking Keep my email addresses private checkbox</li> <li>Go to Profile tab: fill in Name and Public email fields</li> </ol> </li> <li> <p>Open the app's web page using the following link:</p> <p><code>https://&lt;EXTERNAL_IP_FROM_SERVICE&gt;/vf/ui/</code></p> </li> <li> <p>See the guide on how to work with the Visual Flow at the following link: Visual_Flow_User_Guide.pdf</p> </li> <li> <p>For each project Visual Flow (VF) generates a new namespace. </p> </li> </ol> <p>IMPORTANT: For each namespace there is a PVC that will be created and assigned automatically (<code>vf-pvc</code>) in RWX mode (<code>read\\write-many</code>). AKS has default storage clases to provision PVs in RWX mode such as Azure files, but it uses CIFS protocol that does not allow to change file permissions of the files in that PV (https://learn.microsoft.com/en-us/troubleshoot/azure/azure-kubernetes/could-not-change-permissions-azure-files). This option is required for Visual Flow volumes (temporary files), so Azure files could not be used for stable work. We recommend to install third-party storage class in RWX mode, such as rook-ceph to be able to work with files in RWX mode. You can read about how to install rook-ceph on AKS cluster here: Rook_ceph_on_AKS_guide.md</p>"},{"location":"Getting-started-with-Visual-Flow/azure/azure/#stop-start-aks-cluster","title":"Stop \\ Start AKS cluster","text":"<ol> <li> <p>If you want to stop temporary your AKS cluster and VF application, you can simply stop the cluster:</p> <p><code>az aks stop --name &lt;YOUR_CLUSTER_NAME&gt; --resource-group &lt;YOUR_RESOURCE_GROUP_NAME&gt;</code></p> </li> <li> <p>Once you need it back:</p> <p><code>az aks start --name &lt;YOUR_CLUSTER_NAME&gt; --resource-group &lt;YOUR_RESOURCE_GROUP_NAME&gt;</code></p> </li> </ol>"},{"location":"Getting-started-with-Visual-Flow/azure/azure/#delete-visual-flow","title":"Delete Visual Flow","text":"<ol> <li> <p>If the app is no longer required, you can delete it using the following command:</p> <p><code>helm uninstall vf-app -n &lt;VF_NAMESPACE&gt;</code></p> </li> <li> <p>Check that everything was successfully deleted with the command:</p> <p><code>kubectl get pods -n &lt;VF_NAMESPACE&gt;</code></p> </li> <li> <p>Delete Visual Flow namespace:      <code>kubectl delete namespace &lt;VF_NAMESPACE&gt;</code></p> </li> </ol>"},{"location":"Getting-started-with-Visual-Flow/azure/azure/#delete-aks","title":"Delete AKS","text":"<ol> <li> <p>If the AKS is no longer required, you can delete it using the following guide:</p> <p>https://learn.microsoft.com/en-us/cli/azure/aks?view=azure-cli-latest#az-aks-delete</p> </li> </ol>"},{"location":"Getting-started-with-Visual-Flow/google/GCP_NFS_server_how_to/","title":"GCP NFS server how to","text":""},{"location":"Getting-started-with-Visual-Flow/google/GCP_NFS_server_how_to/#steps-to-create-nfs-on-google-cloud-platform-gcp","title":"Steps to create NFS on Google Cloud Platform (GCP):","text":"<ol> <li> <p>Create a Virtual Machine (VM) on GCP      <code>bash      gcloud compute instances create vf-nfs-server \\      --boot-disk-size=20GB \\      --image=ubuntu-minimal-2204-jammy-v20230617 \\      --image-project=ubuntu-os-cloud \\      --machine-type=f1-micro \\      --tags=vf-nfs \\      --zone=us-central1-b</code> </p> </li> <li> <p>Connect via SSH. press yes for everything if asked about different   zone press n, so it autodetect your vm zone      <code>bash      gcloud compute ssh vf-nfs-server</code></p> </li> <li> <p>Install NFS (inside VM)      <code>bash      sudo apt update      sudo apt install -y nfs-kernel-server</code></p> </li> <li> <p>Create a folder for share      <code>bash      sudo mkdir /share      sudo chown nobody:nogroup /share      sudo chmod 777 /share</code></p> </li> <li> <p>Add this foulder to nfs exports</p> <p>Install vim (or any other tool for text edit) on your VM  <code>bash  sudo apt install -y vim</code></p> <p>Edit exports  <code>bash  sudo vim /etc/exports</code></p> <p>Add the next line to /etc/exports  <code>bash  /share *(rw,sync,no_subtree_check)</code></p> </li> <li> <p>Restart nfs-kernel-server      <code>bash      sudo systemctl restart nfs-kernel-server</code></p> <p>Confirm the directory is being shared  <code>bash  sudo exportfs</code>  Output should be like <code>/share world</code>.</p> <p>Done, log out from the machine  <code>bash  exit</code></p> </li> <li> <p>(Optional) if you want to use NFS outside of your google project. Add firewall rules      <code>bash      gcloud compute firewall-rules create nfs \\      --allow=tcp:111,udp:111,tcp:2049,udp:2049 --target-tags=nfs</code></p> </li> </ol>"},{"location":"Getting-started-with-Visual-Flow/google/google/","title":"Installation Visual Flow to Google Kubernetes Engine (GKE)","text":"<ol> <li>Prerequisite Installation<ul> <li>Setting up prerequisite tools</li> <li>Clone Visual Flow repository</li> <li>Create GKE cluster</li> <li>Configure GitHub OAuth</li> <li>Install Redis &amp; PostgreSQL</li> </ul> </li> <li>Installation of Visual Flow</li> <li>Use Visual Flow</li> <li>Stop \\ Start GKE cluster</li> <li>Delete Visual Flow</li> </ol>"},{"location":"Getting-started-with-Visual-Flow/google/google/#prerequisite-installation","title":"Prerequisite Installation","text":"<p>[!NOTE] If you have any concerns, please contact us: info@visual-flow.com</p>"},{"location":"Getting-started-with-Visual-Flow/google/google/#setting-up-prerequisite-tools","title":"Setting up prerequisite tools","text":"<p>To install Visual Flow on GKE you should have the following software on your local\\master machine already installed:</p> <ul> <li>Google CLI (install)</li> <li>kubectl (install)</li> <li>gke-gcloud-auth-plugin (install)</li> <li>Helm CLI (install)</li> <li>Git (install)</li> </ul> <p>[!IMPORTANT] All the actions are recommended to be performed from the Google account with \"Project: Owner\" privileges.</p> <p>If you have just installed the Google CLI, then you need to log in using the following command:</p> <pre><code>gcloud auth login\n</code></pre>"},{"location":"Getting-started-with-Visual-Flow/google/google/#clone-visual-flow-repository","title":"Clone Visual Flow repository","text":"<p>Clone (or download) the google branch from Visual-Flow-deploy repository on your local computer using the following command:</p> <pre><code>git clone -b google https://github.com/ibagroup-eu/Visual-Flow-deploy.git Visual-Flow-GCP-deploy\ncd Visual-Flow-GCP-deploy\n</code></pre>"},{"location":"Getting-started-with-Visual-Flow/google/google/#create-gke-cluster","title":"Create GKE cluster","text":"<p>[!IMPORTANT] If you are new to GKE, please read about Google cloud cluster: cluster types, config params and pricing (https://cloud.google.com/kubernetes-engine/docs/concepts/types-of-clusters)</p> <p>Visual Flow should be installed on GKE cluster. We recommend to use Standard cluster, because Autopilot cluster has some extra limitations and worse application performance. You can create GKE cluster using the following commands:</p> <pre><code>export CLUSTER_NAME=visual-flow\nexport ZONE_NAME=us-central1-b\nexport NUM_NODES=2\ngcloud container clusters create $CLUSTER_NAME --region $ZONE_NAME --num-nodes=$NUM_NODES\n\n# check access\nkubectl get nodes\nkubectl get pods --all-namespaces\n</code></pre> <p>[!TIP] For additional info check the following guide:</p> <p>https://cloud.google.com/kubernetes-engine/docs/how-to/creating-a-zonal-cluster</p>"},{"location":"Getting-started-with-Visual-Flow/google/google/#connect-to-existing-gke-cluster-from-the-local-machine","title":"Connect to existing GKE cluster from the local machine","text":"<p>If you already have GKE cluster, you can connect to it using the following command:</p> <pre><code>gcloud container clusters get-credentials &lt;CLUSTER_NAME&gt; --zone &lt;ZONE_NAME&gt; --project &lt;GOOGLE_PROJECT_NAME&gt;\n</code></pre>"},{"location":"Getting-started-with-Visual-Flow/google/google/#configure-github-oauth","title":"Configure GitHub OAuth","text":"<ol> <li>Go to GitHub user's OAuth apps (<code>https://github.com/settings/developers</code>) or organization's OAuth apps (<code>https://github.com/organizations/&lt;ORG_NAME&gt;/settings/applications</code>)</li> <li>Click the Register a new application or the New Org OAuth App button</li> <li>Fill in the required fields:<ul> <li>Set Homepage URL to <code>https://visual-flow-dummy-url.com/vf/ui/</code></li> <li>Set Authorization callback URL to <code>https://visual-flow-dummy-url.com/vf/ui/callback</code></li> </ul> </li> <li>Click the Register application button</li> <li>Click Generate a new client secret</li> <li>Replace \"DUMMY_ID\" and \"DUMMY_SECRET\" with your <code>Client ID\\Client secret</code> pair value in values.yaml.</li> </ol> <p>[!NOTE] Make sure to copy client secret before you refresh or close the web page. The value will be hidden. In case you lost your client secret, just create a new <code>Client ID\\Client secret</code> pair.</p> <p><code>visual-flow-dummy-url.com</code> is a dummy URL. After Install do not forget to update Homepage URL and Authorization callback URL fields.</p>"},{"location":"Getting-started-with-Visual-Flow/google/google/#install-redis-postgresql","title":"Install Redis &amp; PostgreSQL","text":"<p>Some functionality of VF app requires to have Redis &amp; PosgreSQL dbs. Both of them with custom and default configs included in installation as a separate helm charts (values files with source from bitnami repo). </p> <p>https://github.com/ibagroup-eu/Visual-Flow-deploy/tree/amazon/charts/dbs</p> <p>You can get them and install on you cluster using the following commands:</p> <p>Add 'bitnami' repository to helm repo list</p> <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\n</code></pre> <ol> <li>Redis (for Session and Job's execution history)</li> </ol> <p><code>helm install redis -f charts/dbs/bitnami-redis/values.yaml bitnami/redis</code></p> <ol> <li>PostgreSQL (History service)</li> </ol> <p><code>helm install pgserver -f charts/dbs/bitnami-postgresql/values.yaml bitnami/postgresql</code></p> <p>FYI: Just in case, it is better to save output of these commands (it contains helpful info with short guide, how to get access to pod &amp; dbs and show default credentials).</p>"},{"location":"Getting-started-with-Visual-Flow/google/google/#install-visual-flow","title":"Install Visual Flow","text":"<p>[!NOTE] Current installation is configured to work on <code>default</code> namespace of your cluster. But your Visual Flow projects are stored in <code>vf-&lt;projectname&gt;</code> namespaces.</p> <ol> <li> <p>Go to the directory \"visual-flow\" of the downloaded \"Visual-Flow-Deploy\" repository with the following command:</p> <p><code>bash cd charts/visual-flow</code></p> </li> <li> <p>(Optional) Configure Slack notifications (replace <code>YOUR_SLACK_TOKEN</code>) in values.yaml using the following guide:</p> <p>https://github.com/ibagroup-eu/Visual-Flow-deploy/blob/main/SLACK_NOTIFICATION.md</p> </li> <li> <p>Set superusers in values.yaml.</p> <p>New Visual Flow users will have no access in the app. The superusers(admins) need to be configured to manage user access. Specify the superusers real GitHub nicknames in values.yaml in the yaml list format:</p> <p><code>yaml superusers:   - your-github-nickname   # - another-superuser-nickname</code></p> </li> <li> <p>If you have installed kube-metrics then update values.yaml file according to the example below.</p> <ol> <li> <p>Check that the kube-metrics installed using the following command:</p> <p><code>bash kubectl top pods</code></p> <p>Output if the kube-metrics isn't installed:</p> <p><code>error: Metrics API not available</code></p> <p>If the kube-metrics isn't installed then go to step 6.</p> </li> <li> <p>Edit values.yaml file according to the example below:</p> <p><code>yaml ... kube-metrics:   install: false</code></p> </li> </ol> </li> <li> <p>If you have installed Argo workflows then update values.yaml file according to the example below.</p> <ol> <li> <p>Check that the Argo workflows installed using the following command:</p> <p><code>bash kubectl get workflow</code></p> <p>Output if the Argo workflows isn't installed:</p> <p><code>error: the server doesn't have a resource type \"workflow\"</code></p> <p>If the Argo workflows isn't installed then go to step 7.</p> </li> <li> <p>Edit values.yaml file according to the example below:</p> <p><code>yaml ... argo:   install: false vf-app:   backend:     configFile:       argoServerUrl: &lt;Argo-Server-URL&gt;</code></p> </li> </ol> </li> <li> <p>Install the app using the updated values.yaml file with the following command:</p> <p><code>bash helm upgrade -i vf-app . -f values.yaml -n default</code></p> </li> <li> <p>Check that the app is successfully installed and all pods are running with the following command:</p> <p><code>bash kubectl get pods --all-namespaces</code></p> </li> <li> <p>Get the generated app's hostname\\IP with the following command:</p> <p><code>bash kubectl get svc visual-flow-frontend -n default -o yaml | grep -i clusterIP: | cut -c 14-</code></p> <p>Replace the string <code>&lt;HOSTNAME_FROM_SERVICE&gt;</code> with the generated hostname\\IP in the next steps. Replace <code>visual-flow-dummy-url.com</code> from OAuth step with your hostname or IP in Homepage URL and Authorization callback URL fields. Save changes.</p> </li> <li> <p>Update 'host' (<code>host: https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/</code>) and 'STRATEGY_CALLBACK_URL' (<code>STRATEGY_CALLBACK_URL: https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/callback</code>) values in values.yaml. </p> </li> <li> <p>Upgrade the app in EKS cluster using updated values.yaml:</p> <p><code>bash helm upgrade vf-app . -f values.yaml -n default</code></p> </li> <li> <p>Wait until the update is installed and all pods are running:</p> <p><code>bash kubectl get pods --all-namespaces</code></p> </li> </ol>"},{"location":"Getting-started-with-Visual-Flow/google/google/#use-visual-flow","title":"Use Visual Flow","text":"<ol> <li> <p>All Visual Flow users (including superusers) need active Github account in order to be authenticated in application. Setup Github profile as described in the next steps:</p> <ol> <li>Navigate to the account settings</li> <li>Go to Emails tab: set email as public by unchecking Keep my email addresses private checkbox</li> <li>Go to Profile tab: fill in Name and Public email fields</li> </ol> </li> <li> <p>Open the app's web page using the following link:</p> <p><code>https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/</code></p> </li> <li> <p>See the guide on how to work with the Visual Flow at the following link: Visual_Flow_User_Guide.pdf</p> </li> <li> <p>For each project Visual Flow (VF) generates a new namespace. </p> </li> </ol> <p>[!IMPORTANT] For each namespace there is a PVC that will be created and assigned automatically (<code>vf-pvc</code>) in RWX mode (<code>read\\write-many</code>). GKE has default storage classes to provision PV in RWX modes (f.e. <code>standard-rwx</code>) that uses Cloud Filestore API service, but it has limitation size (1Tb-10Tb) when VF usually use small disks (f.e. 2G per project\\namespace). So, each VF project will cost you at least 200$/month. There is an open ticket for this feature: (https://issuetracker.google.com/issues/193108375?pli=1). Instead, you can use NFS and assign it to each new namespace, but it have to be assigned manually for each VF project. You can read here about how to create yourself NFS server on your Google cloud (https://github.com/ibagroup-eu/Visual-Flow-deploy/blob/google/GCP_NFS_server_how_to.md). If you do not need to know how to create a new NFS server and assign VF to it you can skip this section.</p> <p>First, create the project in the app, open it and check the URL of the page. It will have the following format:</p> <p><code>https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/&lt;NAMESPACE&gt;/overview</code></p> <p>Now delete automatically created PVC in that : <p><code>bash    kubectl delete pvc vf-pvc -n &lt;NAMESPACE&gt;</code></p> <p>Once you have your NFS server ready, create a file with next content and replace items from comments in the header.</p> <pre><code>```yaml\n# &lt;PV_NAME&gt; - name of Persistent Volume for your project. # vf-pvc-testing\n# &lt;STORAGE_SIZE&gt; - storage size that you want to assign to this VF project. # 2G\n# &lt;NFS_HOST&gt; - NFS server ip. # YOUR_NFS_IP\n# &lt;NFS_PATH&gt; - PATH to shared folder in your NFS you want to use in this VF project. # /share\n# &lt;NAMESPACE&gt; - VF project namespace for jobs. # vf-test\napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: &lt;PV_NAME&gt;\nspec:\n  storageClassName: \"\"\n  persistentVolumeReclaimPolicy: Delete\n  capacity:\n    storage: &lt;STORAGE_SIZE&gt;\n  accessModes:\n    - ReadWriteMany\n  nfs:\n    server: &lt;NFS_HOST&gt;\n    path: &lt;NFS_PATH&gt;\n---   \napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: vf-pvc\n  namespace: &lt;NAMESPACE&gt;\nspec:\n  storageClassName: \"\"\n  volumeName: &lt;PV_NAME&gt;\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: &lt;STORAGE_SIZE&gt;\n```\n\nDeploy your file using next command:\n\n```bash\nkubectl apply -f &lt;your_yaml_file_with_pvc&gt; -n &lt;NAMESPACE&gt;\n```\n</code></pre>"},{"location":"Getting-started-with-Visual-Flow/google/google/#stop-start-gke-cluster","title":"Stop \\ Start GKE cluster","text":"<ol> <li> <p>If you want to stop temporary your GKE cluster and VF application, the easiest way is to scale down the number of nodes:</p> <p><code>bash gcloud container clusters resize visual-flow --node-pool default-pool --num-nodes 0 --zone &lt;ZONE_NAME&gt;</code></p> </li> <li> <p>Once you need it back, just restore num-nodes back:</p> <p><code>bash gcloud container clusters resize visual-flow --node-pool default-pool --num-nodes &lt;NUM_NODES&gt; --zone &lt;ZONE_NAME&gt;</code></p> </li> </ol>"},{"location":"Getting-started-with-Visual-Flow/google/google/#delete-visual-flow","title":"Delete Visual Flow","text":"<ol> <li> <p>If the app is no longer required, you can delete it using the following command:</p> <p><code>bash helm uninstall vf-app -n default</code></p> </li> <li> <p>Check that everything was successfully deleted with the command:</p> <p><code>bash kubectl get pods -n default</code></p> </li> </ol>"},{"location":"Getting-started-with-Visual-Flow/google/google/#delete-gke","title":"Delete GKE","text":"<ol> <li> <p>If the GKE is no longer required, you can delete it using the following guide:</p> <p>https://cloud.google.com/sdk/gcloud/reference/container/clusters/delete</p> </li> </ol>"},{"location":"Getting-started-with-Visual-Flow/minikube/minikube/","title":"Installation Visual Flow to Local Minikube","text":""},{"location":"Getting-started-with-Visual-Flow/minikube/minikube/#prerequisites","title":"Prerequisites","text":"<p>To install Visual Flow you should have the following software installed:</p> <ul> <li>Git (install)</li> <li>kubectl (install)</li> <li>Helm CLI (install)</li> <li>Minikube (install)</li> </ul> <p>By default VF projects require 4 CPU and 6GB of RAM. We do not recommend to reduce these settings (to avoid job run issues), but you may increase these values based on your workload.</p>"},{"location":"Getting-started-with-Visual-Flow/minikube/minikube/#create-minikube-cluster","title":"Create Minikube cluster","text":"<p>In this example we will use the HyperV VM driver, which is recommended for the Windows OS family. But depending on your system or requirements - you can also use Docker, VirtualBox, Podman, KVM2 etc.</p> <p>Also kubernetes version is 1.25.4, since current latest one (1.27.2) caused problem with GitOAuth Authentification (you may get issue like 'Failed to obtain access token'). So at least on this version with HyperV driver app was tested and works without any problem.</p> <p>You can create simple cluster in Minikube using following commands:</p> <pre><code>minikube start --cpus 4 --memory 6g --disk-size 20g --delete-on-failure=true --driver hyperv --kubernetes-version=v1.25.4 -p visual-flow \n\n# duration: ~5-10min\n\n# if creation failed - delete cluster using following command and repeat from beginning:\n\n#&gt; minikube delete -p visual-flow\n</code></pre> <p>When cluster is ready - you can switch default profile to this cluster, check running pods and cluster IP:</p> <pre><code>minikube profile visual-flow\n\nkubectl get pods -A\n\nminikube ip\n# on this IP will be available VF application and other services\n</code></pre> <p>For additional info about Minikube check following guide:  https://minikube.sigs.k8s.io/docs/start</p>"},{"location":"Getting-started-with-Visual-Flow/minikube/minikube/#install-redis-postgresql","title":"Install Redis &amp; PostgreSQL","text":"<p>Some functionality of VF app requires to have Redis &amp; PosgreSQL dbs. Both of them with custom and default configs included in installation as a separate helm charts (values files with source from bitnami repo). </p> <p>https://github.com/ibagroup-eu/Visual-Flow-deploy/tree/minikube/charts/dbs</p> <p>You can get them and install on you cluster using following actions.</p> <ul> <li>Add 'bitnami' repository to helm repo list</li> </ul> <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm repo update\n</code></pre> <ul> <li>Clone Minikube branch from Visual-Flow-deploy repository and go to Visual-Flow-deploy/charts/dbs</li> </ul> <pre><code>git clone -b minikube https://github.com/ibagroup-eu/Visual-Flow-deploy.git Visual-Flow-deploy\n\ncd Visual-Flow-deploy/charts/dbs\n</code></pre> <ol> <li>Redis (for Session and Job's execution history)</li> </ol> <pre><code>helm install redis -f bitnami-redis/values.yaml bitnami/redis\n</code></pre> <ol> <li>PostgreSQL (History service)</li> </ol> <pre><code>helm install pgserver -f bitnami-postgresql/values.yaml bitnami/postgresql\n</code></pre> <ul> <li>Check that both services Ready and Running</li> </ul> <pre><code>&gt; kubectl get pods\nNAME                    READY   STATUS    RESTARTS   AGE\npgserver-postgresql-0   1/1     Running   0          2m59s\nredis-master-0          1/1     Running   0          3m23s\n</code></pre> <p>FYI: Just in case better to save output of these command (it contains helpful info with short guide how to get access to pod &amp; dbs and show default credentials).</p> <p>Go back to your main folder to proceed with Visual Flow installation</p> <pre><code>cd ../../..\n</code></pre>"},{"location":"Getting-started-with-Visual-Flow/minikube/minikube/#install-visual-flow","title":"Install Visual Flow","text":"<ol> <li>Make sure you already have the Minikube branch from Visual-Flow-deploy repository on your local computer, if no - clone it using the following command:</li> </ol> <pre><code>git clone -b minikube https://github.com/ibagroup-eu/Visual-Flow-deploy.git Visual-Flow-deploy\n</code></pre> <ol> <li>Go to the directory \"visual-flow\" of the downloaded \"Visual-Flow-Deploy\" repository with the following command:</li> </ol> <pre><code>cd Visual-Flow-deploy/charts/visual-flow\n</code></pre> <ol> <li>(Optional). Configure Slack notifications in values.yaml using following guide:</li> </ol> <p>Configure Slack notification</p> <ol> <li> <p>Set superusers in values.yaml.</p> <p>New Visual Flow users will have no access in the app. The superusers(admins) need to be configured to manage user access. Specify the superusers real GitHub nicknames in values.yaml in the yaml list format:</p> <p><code>yaml superusers:   - your-github-nickname   # - another-superuser-nickname</code></p> </li> <li> <p>If you have installed kube-metrics then update values.yaml file according to the example below.</p> <ol> <li> <p>Check that the kube-metrics installed using the following command:</p> <p><code>bash kubectl top pods</code></p> <p>Output if the kube-metrics isn't installed:</p> <p><code>error: Metrics API not available</code></p> <p>If the kube-metrics isn't installed then go to step 6.</p> </li> <li> <p>Edit values.yaml file according to the example below:</p> <p><code>yaml ... kube-metrics:   install: false</code></p> </li> </ol> </li> <li> <p>If you have installed Argo workflows then update values.yaml file according to the example below.</p> <ol> <li> <p>Check that the Argo workflows installed using the following command:</p> <p><code>bash kubectl get workflow</code></p> <p>Output if the Argo workflows isn't installed:</p> <p><code>error: the server doesn't have a resource type \"workflow\"</code></p> <p>If the Argo workflows isn't installed then go to step 7.</p> </li> <li> <p>Edit values.yaml file according to the example below:</p> <p><code>yaml ... argo:   install: false vf-app:   backend:     configFile:       argoServerUrl: &lt;Argo-Server-URL&gt;</code></p> </li> </ol> </li> <li> <p>Install the app using the updated values.yaml file with the following command:</p> <p><code>bash helm upgrade -i vf-app . -f values.yaml</code></p> </li> <li> <p>Check that the app is successfully installed and all pods are running with the following command:</p> <p><code>bash kubectl get pods -A</code></p> </li> <li> <p>Get the IP of your cluster with following command:</p> <p><code>bash minikube ip</code></p> <p>Replace the string <code>&lt;HOSTNAME_FROM_SERVICE&gt;</code> with the generated hostname in the next steps.</p> </li> <li> <p>Create a GitHub OAuth app:</p> <ol> <li>Go to GitHub user's OAuth apps (<code>https://github.com/settings/developers</code>) or organization's OAuth apps (<code>https://github.com/organizations/&lt;ORG_NAME&gt;/settings/applications</code>).</li> <li>Click the Register a new application or the New OAuth App button.</li> <li>Fill the required fields:<ul> <li>Set Homepage URL to <code>https://&lt;HOSTNAME_FROM_SERVICE&gt;:30910/vf/ui/</code></li> <li>Set Authorization callback URL to <code>https://&lt;HOSTNAME_FROM_SERVICE&gt;:30910/vf/ui/callback</code></li> </ul> </li> <li>Click the Register application button.</li> <li>Replace \"DUMMY_ID\" with the Client ID value in values.yaml.</li> <li>Click Generate a new client secret and replace in values.yaml \"DUMMY_SECRET\" with the generated Client secret value (Please note that you will not be able to see the full secret value later).</li> </ol> </li> <li> <p>Update 'uiHost' (<code>uiHost: https://&lt;HOSTNAME_FROM_SERVICE&gt;</code>) and 'STRATEGY_CALLBACK_URL' (<code>STRATEGY_CALLBACK_URL: https://&lt;HOSTNAME_FROM_SERVICE&gt;/vf/ui/callback</code>) values in values.yaml. </p> </li> <li> <p>Upgrade release using updated 'values.yaml':</p> <p><code>bash helm upgrade vf-app . -f values.yaml</code></p> </li> <li> <p>Wait until the update is installed and all pods are running:</p> <p><code>bash kubectl get pods -A</code></p> </li> </ol>"},{"location":"Getting-started-with-Visual-Flow/minikube/minikube/#use-visual-flow","title":"Use Visual Flow","text":"<ol> <li> <p>All Visual Flow users (including superusers) need active Github account in order to be authenticated in application. Setup Github profile as per following steps:</p> <ol> <li>Navigate to the account settings</li> <li>Go to Emails tab: set email as public by unchecking Keep my email addresses private checkbox</li> <li>Go to Profile tab: fill in Name and Public email fields</li> </ol> </li> <li> <p>Open the app's web page using the following link:</p> <p><code>https://&lt;HOSTNAME_FROM_SERVICE&gt;:30910/vf/ui/</code></p> </li> <li> <p>See the guide on how to work with the Visual Flow at the following link: Visual_Flow_User_Guide.pdf</p> </li> </ol>"},{"location":"Getting-started-with-Visual-Flow/minikube/minikube/#delete-visual-flow","title":"Delete Visual Flow","text":"<ol> <li> <p>If the app is no longer need, you can delete it using the following command:</p> <p><code>helm uninstall vf-app</code></p> </li> <li> <p>Check that everything was successfully deleted with the command:</p> <p><code>kubectl get pods --all-namespaces</code></p> </li> </ol>"},{"location":"Getting-started-with-Visual-Flow/minikube/minikube/#delete-additional-components","title":"Delete additional components","text":"<p>If you do no need them anymore - you can also delete and these additional components:</p> <ul> <li>Redis &amp; PostgreSQL databases</li> </ul> <p><code>helm uninstall redis</code></p> <p><code>helm uninstall pgserver</code></p>"},{"location":"Getting-started-with-Visual-Flow/minikube/minikube/#delete-minikube-cluster-and-profile","title":"Delete Minikube cluster and profile","text":"<p>If this cluster is no longer need - you can delete it using the following command:</p> <p><code>minikube delete -p visual-flow</code></p>"},{"location":"Getting-started-with-Visual-Flow/minikube/minikube/#helpful-links-about-minikube","title":"Helpful links about Minikube:","text":"<ul> <li>Minikube Start (https://minikube.sigs.k8s.io/docs/start)</li> <li>Minikube Basic Control (https://minikube.sigs.k8s.io/docs/handbook/controls)</li> <li>Minikube Dashboard (https://minikube.sigs.k8s.io/docs/handbook/dashboard)</li> <li>Minikube Tutorials (https://minikube.sigs.k8s.io/docs/tutorials)</li> <li>Minikube FAQ (https://minikube.sigs.k8s.io/docs/faq)</li> </ul>"},{"location":"Visual-Flow-services/Backend/","title":"About Visual Flow","text":"<p>Visual Flow is an ETL/ELT tool designed for effective data management via convenient and user-friendly interface. The tool has the following capabilities:</p> <ul> <li>Can integrate data from heterogeneous sources:</li> <li>Azure Blob Storage</li> <li>AWS S3</li> <li>Cassandra</li> <li>Click House</li> <li>DB2</li> <li>Databricks JDBC (global configuration)</li> <li>Databricks (Databricks configuration)</li> <li>Dataframe (for reading)</li> <li>Google Cloud Storage</li> <li>Elastic Search</li> <li>IBM COS</li> <li>Kafka</li> <li>Local File</li> <li>MS SQL</li> <li>Mongo</li> <li>MySQL/Maria</li> <li>Oracle</li> <li>PostgreSQL</li> <li>Redis</li> <li>Redshift</li> <li>REST API</li> <li>It supports the following file formats:</li> <li>Delta Lake</li> <li>Parquet</li> <li>JSON</li> <li>CSV</li> <li>ORC</li> <li>Avro</li> <li>Text</li> <li>Binary (PDF, DOC, Audio files)</li> <li>Leverage direct connectivity to enterprise applications as sources and targets</li> <li>Perform data processing and transformation</li> <li>Run custom code</li> <li>Leverage metadata for analysis and maintenance</li> <li>Allows to deploy in two configurations and run jobs in Spark/Kubernetes and Databricks environments respectively</li> <li>Leverages Generative AI capabilities via tasks like Parse text, Generate data, Transcribe, Generic task</li> </ul> <p>Visual Flow application is divided into the following repositories:</p> <ul> <li>Visual-Flow-frontend</li> <li>Visual-Flow-backend (current)</li> <li>Visual-Flow-jobs</li> <li>Visual-Flow-deploy</li> <li>Visual-Flow-backend-db-service</li> <li>Visual-Flow-backend-history-service</li> </ul>"},{"location":"Visual-Flow-services/Backend/#visual-flow-backend","title":"Visual Flow Backend","text":"<p>Visual Flow backend is the REST API app, that serves as a middleware between frontend application and k8s-like orchestration environments, that run jobs with Spark. It gives you ability to manage Visual Flow entities (projects, jobs, pipelines) in the following ways:</p> <ul> <li>Create/delete project which serves as a namespace for jobs and/or pipelines</li> <li>Manage project settings</li> <li>User access management</li> <li>Create/maintain a job</li> <li>Job execution and logs analysis</li> <li>Create/maintain a pipeline</li> <li>Pipeline execution</li> <li>Cron pipelines</li> <li>Import/Export jobs and pipelines</li> </ul>"},{"location":"Visual-Flow-services/Backend/#development","title":"Development","text":"<p>Check the official guide.</p>"},{"location":"Visual-Flow-services/Backend/#contribution","title":"Contribution","text":"<p>Check the official guide.</p>"},{"location":"Visual-Flow-services/Backend/#license","title":"License","text":"<p>Visual Flow is an open-source software licensed under the Apache-2.0 license.</p>"},{"location":"Visual-Flow-services/Backend/DEVELOPMENT/","title":"Set up development environment","text":""},{"location":"Visual-Flow-services/Backend/DEVELOPMENT/#requirements","title":"Requirements","text":"<p>1) Make sure that you have the following software installed</p> Software Version Purpose Java 11+ To work with this API as it has been written in Java Maven 3+ To manage project dependencies openssl any recent To manage certificates for ssl Kubernetes CLI actual for target cluster To connect to Kubernetes <p>2) Github account and access token with READ permissions/scope. It is needed to download one of the project's dependencies - argo-client-java. See pom.xml and settings.xml for more info</p> <p>3) Following environment variables have to be set</p> Name Explanation M2_HOME To be able to execute Maven commands within shell BASE_PATH The sub path after hostname on which API will be available GITHUB_USERNAME The username of your Github account that you've created access token for GITHUB_TOKEN Github access token, needed to download java argo client dependency KEYSTORE_PASS Password for both p12 key store and a key within it ARGO_SERVER_URL URL to argo web server SLACK_API_TOKEN API token for Slack, needed to send out notifications"},{"location":"Visual-Flow-services/Backend/DEVELOPMENT/#ssl-configuration","title":"SSL configuration","text":"<p>Our API requires SSL certificate, and a private key packed up in p12 store.</p> <p>1) Get a certificate, and a private key.    If you want to roll with self-signed one(which is enough for development), you can use the command below:    <code>bash    openssl req -x509 -out &lt;PATH_TO_CRT_FILE&gt; -keyout &lt;PATH_TO_KEY_FILE&gt; -newkey rsa:2048 -nodes -sha256 -days 365 -subj '/CN=localhost'</code></p> <p>2) Put your certificate and a private key into p12 keystore:    <code>bash    openssl pkcs12 -export -in &lt;PATH_TO_CRT_FILE&gt; -inkey &lt;PATH_TO_KEY_FILE&gt; -name &lt;KEY_ALIAS&gt; -out &lt;PATH_TO_P12_FILE&gt; -password pass:$KEYSTORE_PASS</code></p> <p>Note that in both commands we've used some placeholders. These are meant to be replaced with actual values for your system:</p> <ul> <li> - path to a new .crt file <li> - path to a new .key file <li> - alias of the key, meant to be referenced in server.ssl.key-alias within application.yaml <li> - path to a new .p12 key store file"},{"location":"Visual-Flow-services/Backend/DEVELOPMENT/#kubernetes","title":"Kubernetes","text":"<p>Visual Flow app requires a Kubernetes cluster to run. You must have a kubernetes configuration file with the correct information to connect to an existing kubernetes cluster. You can install kubernetes cli and login to the cluster through it to generate a config file. Make sure that user, which you are logged in, has permission to list all namespaces at least. But we recommend to have cluster-admin role, to allow backend create and delete projects(namespaces in kubernetes).</p>"},{"location":"Visual-Flow-services/Backend/DEVELOPMENT/#argo-workflows","title":"Argo Workflows","text":"<p>Argo Workflows engine should be installed in target kubernetes cluster. Visual Flow app requires argo workflows web server to work with pipelines. You can deploy server to target kubernetes cluster or start it locally with argo workflows cli. You should put url to argo workflows web server to argo.serverUrl field in application.yaml file. How to install and configure argo worklows: Visual-Flow-deploy/README.md#argo-workflows</p>"},{"location":"Visual-Flow-services/Backend/DEVELOPMENT/#spark","title":"Spark","text":"<p>Visual Flow app uses spark job to work with data. All required tools to run spark jobs located in spark job docker image. No additional services are required.</p>"},{"location":"Visual-Flow-services/Backend/DEVELOPMENT/#application-configuration","title":"Application configuration","text":"<p>Now, when you are done with everything described above, it's time to create application.yaml Take a look at application.yaml.example. It will serve as a template to your future configuration.</p> <ol> <li>Create a new application.yaml file inside src/main/resources/ and fill it with the contents from application.yaml.example.</li> <li>Change and fill values within configuration. Existing comments can help to understand what is required.</li> </ol>"},{"location":"Visual-Flow-services/Backend/DEVELOPMENT/#how-to-run-and-use","title":"How to run and use","text":"<ol> <li>Login to Kubernetes server if not logged in yet.</li> <li>Get the access token from the OAuth server (server's URL would be specified in application.yaml).</li> <li>Build and run the application.</li> <li>Make requests to API endpoints:<ul> <li>User info:   <code>bash   curl --location --request GET 'https://localhost:8080/${BASE_PATH}/api/user' \\   --header 'Authorization: Bearer &lt;OAUTH_TOKEN&gt;'</code></li> <li>Project list:   <code>bash   curl --location --request GET 'https://localhost:8080/${BASE_PATH}/api/project' \\         --header 'Authorization: Bearer &lt;OAUTH_TOKEN&gt;'</code></li> </ul> </li> </ol> <p>If you are using self-signed certificate, yon can see warning that server is not trusted. Just use your-client-based option to ignore that warning.  is your access token that you've retrieved on the second step of this section."},{"location":"Visual-Flow-services/Backend/DEVELOPMENT/#swagger","title":"Swagger","text":"<p>Visual Flow comes with preconfigured Swagger.</p> <p>You should be able to access swagger by the following URL:</p> <p>https://localhost:8080/${BASE_PATH}/swagger-ui.html</p> <p>To be able to work with endpoints, you would have to be authenticated. Do it by providing oauth bearer token into Authorization header.</p>"},{"location":"Visual-Flow-services/Backend%28databases%29/","title":"About Visual Flow","text":"<p>Visual Flow is an ETL/ELT tool designed for effective data management via convenient and user-friendly interface. The tool has the following capabilities:</p> <ul> <li>Can integrate data from heterogeneous sources:</li> <li>Azure Blob Storage</li> <li>AWS S3</li> <li>Cassandra</li> <li>Click House</li> <li>DB2</li> <li>Databricks JDBC (global configuration)</li> <li>Databricks (Databricks configuration)</li> <li>Dataframe (for reading)</li> <li>Google Cloud Storage</li> <li>Elastic Search</li> <li>IBM COS</li> <li>Kafka</li> <li>Local File</li> <li>MS SQL</li> <li>Mongo</li> <li>MySQL/Maria</li> <li>Oracle</li> <li>PostgreSQL</li> <li>Redis</li> <li>Redshift</li> <li>REST API</li> <li>It supports the following file formats:</li> <li>Delta Lake</li> <li>Parquet</li> <li>JSON</li> <li>CSV</li> <li>ORC</li> <li>Avro</li> <li>Text</li> <li>Binary (PDF, DOC, Audio files)</li> <li>Leverage direct connectivity to enterprise applications as sources and targets</li> <li>Perform data processing and transformation</li> <li>Run custom code</li> <li>Leverage metadata for analysis and maintenance</li> <li>Allows to deploy in two configurations and run jobs in Spark/Kubernetes and Databricks environments respectively</li> <li>Leverages Generative AI capabilities via tasks like Parse text, Generate data, Transcribe, Generic task</li> </ul> <p>Visual Flow application is divided into the following repositories:</p> <ul> <li>Visual-Flow-frontend</li> <li>Visual-Flow-backend</li> <li>Visual-Flow-jobs</li> <li>Visual-Flow-deploy</li> <li>Visual-Flow-backend-db-service (current)</li> <li>Visual-Flow-backend-history-service</li> </ul>"},{"location":"Visual-Flow-services/Backend%28databases%29/#visual-flow-backend-db-service","title":"Visual Flow Backend DB Service","text":"<p>Visual Flow Backend DB Service is the REST API app, which allows to interact with the databases listed  above using the following operations: - Ping connection.</p> <p>To interact with the microservice, you should use corresponding endpoints in  the main microservice.</p>"},{"location":"Visual-Flow-services/Backend%28databases%29/#development","title":"Development","text":"<p>Check the official guide.</p>"},{"location":"Visual-Flow-services/Backend%28databases%29/#contribution","title":"Contribution","text":"<p>Check the official guide.</p>"},{"location":"Visual-Flow-services/Backend%28databases%29/#license","title":"License","text":"<p>Visual Flow is an open-source software licensed under the Apache-2.0 license.</p>"},{"location":"Visual-Flow-services/Backend%28databases%29/DEVELOPMENT/","title":"Set up development environment","text":""},{"location":"Visual-Flow-services/Backend%28databases%29/DEVELOPMENT/#requirements","title":"Requirements","text":"<p>1) Make sure that you have the following software installed</p> Software Version Purpose Java 11+ To work with this API as it has been written in Java Maven 3+ To manage project dependencies <p>2) Following environment variables have to be set</p> Name Explanation BASE_PATH The sub path after hostname on which API will be available"},{"location":"Visual-Flow-services/Backend%28databases%29/DEVELOPMENT/#ssl-configuration","title":"SSL configuration","text":"<p>This API doesn't require SSL certificate, and a private key packed up in p12 store.</p>"},{"location":"Visual-Flow-services/Backend%28databases%29/DEVELOPMENT/#application-configuration","title":"Application configuration","text":"<p>Now, when you are done with everything described above, it's time to create application.yaml Take a look at application.yaml.example. It will serve as a template to your future configuration.</p> <ol> <li>Create a new application.yaml file inside src/main/resources/ and fill it with the contents from application.yaml.example.</li> <li>Change and fill values within configuration. Existing comments can help to understand what is required.</li> </ol>"},{"location":"Visual-Flow-services/Backend%28databases%29/DEVELOPMENT/#how-to-run-and-use","title":"How to run and use","text":"<ol> <li>Build and run the application.</li> <li>Make sure there is access to resources by making, for example, the following request:<ul> <li>Try to ping:   <code>bash   curl --location --request POST '{host}:{port}/${BASE_PATH}' --header 'Content-Type: application/json' \\   --data-raw '{\"key\": \"key\", \"value\": {}}'</code>   should return false.</li> </ul> </li> </ol>"},{"location":"Visual-Flow-services/Backend%28databases%29/DEVELOPMENT/#notice","title":"Notice","text":"<p>When configuring the Redis database, keep in mind that you have the current version of the Redis server, since versions  older than 2.6 have a vulnerability that is solved only by configuring the server: CVE-2021-32626</p> <p>Redis is an open source, in-memory database that persists on disk. In affected versions specially crafted Lua scripts  executing in Redis can cause the heap-based Lua stack to be overflowed, due to incomplete checks for this condition.  This can result with heap corruption and potentially remote code execution. This problem exists in all versions of  Redis with Lua scripting support, starting from 2.6. The problem is fixed in versions 6.2.6, 6.0.16 and 5.0.14.  For users unable to update an additional workaround to mitigate the problem without patching the redis-server  executable is to prevent users from executing Lua scripts. This can be done using ACL to restrict EVAL and  EVALSHA commands.</p>"},{"location":"Visual-Flow-services/Backend%28databases%29/DEVELOPMENT/#swagger","title":"Swagger","text":"<p>Visual Flow comes with preconfigured Swagger.</p> <p>You should be able to access swagger in main backend microservice, section \"Databases API\".</p>"},{"location":"Visual-Flow-services/Backend%28history%29/","title":"About Visual Flow","text":"<p>Visual Flow is an ETL/ELT tool designed for effective data management via convenient and user-friendly interface. The tool has the following capabilities:</p> <ul> <li>Can integrate data from heterogeneous sources:</li> <li>Azure Blob Storage</li> <li>AWS S3</li> <li>Cassandra</li> <li>Click House</li> <li>DB2</li> <li>Databricks JDBC (global configuration)</li> <li>Databricks (Databricks configuration)</li> <li>Dataframe (for reading)</li> <li>Google Cloud Storage</li> <li>Elastic Search</li> <li>IBM COS</li> <li>Kafka</li> <li>Local File</li> <li>MS SQL</li> <li>Mongo</li> <li>MySQL/Maria</li> <li>Oracle</li> <li>PostgreSQL</li> <li>Redis</li> <li>Redshift</li> <li>REST API</li> <li>It supports the following file formats:</li> <li>Delta Lake</li> <li>Parquet</li> <li>JSON</li> <li>CSV</li> <li>ORC</li> <li>Avro</li> <li>Text</li> <li>Binary (PDF, DOC, Audio files)</li> <li>Leverage direct connectivity to enterprise applications as sources and targets</li> <li>Perform data processing and transformation</li> <li>Run custom code</li> <li>Leverage metadata for analysis and maintenance</li> <li>Allows to deploy in two configurations and run jobs in Spark/Kubernetes and Databricks environments respectively</li> <li>Leverages Generative AI capabilities via tasks like Parse text, Generate data, Transcribe, Generic task</li> </ul> <p>Visual Flow application is divided into the following repositories:</p> <ul> <li>Visual-Flow-frontend</li> <li>Visual-Flow-backend</li> <li>Visual-Flow-jobs</li> <li>Visual-Flow-deploy</li> <li>Visual-Flow-backend-db-service</li> <li>Visual-Flow-backend-history-service (current)</li> </ul>"},{"location":"Visual-Flow-services/Backend%28history%29/#contribution","title":"Contribution","text":"<p>Check the official guide.</p>"},{"location":"Visual-Flow-services/Backend%28history%29/#license","title":"License","text":"<p>Visual flow is an open-source software licensed under the Apache-2.0 license.</p>"},{"location":"Visual-Flow-services/Frontend/","title":"About Visual Flow","text":"<p>Visual Flow is an ETL/ELT tool designed for effective data management via convenient and user-friendly interface. The tool has the following capabilities:</p> <ul> <li>Can integrate data from heterogeneous sources:</li> <li>Azure Blob Storage</li> <li>AWS S3</li> <li>Cassandra</li> <li>Click House</li> <li>DB2</li> <li>Databricks JDBC (global configuration)</li> <li>Databricks (Databricks configuration)</li> <li>Dataframe (for reading)</li> <li>Google Cloud Storage</li> <li>Elastic Search</li> <li>IBM COS</li> <li>Kafka</li> <li>Local File</li> <li>MS SQL</li> <li>Mongo</li> <li>MySQL/Maria</li> <li>Oracle</li> <li>PostgreSQL</li> <li>Redis</li> <li>Redshift</li> <li>REST API</li> <li>It supports the following file formats:</li> <li>Delta Lake</li> <li>Parquet</li> <li>JSON</li> <li>CSV</li> <li>ORC</li> <li>Avro</li> <li>Text</li> <li>Binary (PDF, DOC, Audio files)</li> <li>Leverage direct connectivity to enterprise applications as sources and targets</li> <li>Perform data processing and transformation</li> <li>Run custom code</li> <li>Leverage metadata for analysis and maintenance</li> <li>Allows to deploy in two configurations and run jobs in Spark/Kubernetes and Databricks environments respectively</li> <li>Leverages Generative AI capabilities via tasks like Parse text, Generate data, Transcribe, Generic task</li> </ul> <p>Visual Flow application is divided into the following repositories:</p> <ul> <li>Visual-Flow-frontend (current)</li> <li>Visual-Flow-backend</li> <li>Visual-Flow-jobs</li> <li>Visual-Flow-deploy</li> <li>Visual-Flow-backend-db-service</li> <li>Visual-Flow-backend-history-service</li> </ul>"},{"location":"Visual-Flow-services/Frontend/#visual-flow-frontend","title":"Visual Flow Frontend","text":""},{"location":"Visual-Flow-services/Frontend/#process-overview","title":"Process Overview","text":"<p>Visual Flow jobs and pipelines exist within a certain namespace (project) so the first step in the application would be to create a project or enter existing project. Then you need to enter Job Designer to create a job.</p>"},{"location":"Visual-Flow-services/Frontend/#jobdesigner","title":"JobDesigner","text":"<p>Job designer is a graphical design interface used to create, maintain,execute and analyze jobs. Each job determines the data sources, the required transformations and destination of the data. Designing a pipeline is similar to designing a job.</p>"},{"location":"Visual-Flow-services/Frontend/#pipelinedesigner","title":"PipelineDesigner","text":"<p>Pipeline designer is a graphical design interface aimed for managing pipelines.</p> <p>Visual Flow key functions include but not limited to </p> <ul> <li>Create project which serves as a namespace for jobs and/or pipelines</li> <li>Manage project settings</li> <li>User access management</li> <li>Create/maintain a job in Job Designer</li> <li>Job execution and logs analysis</li> <li>Create/maintain a pipeline in Pipeline Designer</li> <li>Pipeline execution</li> <li>Import/Export jobs and pipelines</li> </ul>"},{"location":"Visual-Flow-services/Frontend/#roles-and-authorizations","title":"Roles and authorizations","text":"<p>The following roles are available in the application:</p> <ul> <li>Viewer</li> <li>Operator</li> <li>Editor</li> <li>Administrator</li> </ul> <p>They can perform the below operations within the namespaces they are authorized to. Only Super-admin user can create a workspace (project) and grant access to this project.</p>"},{"location":"Visual-Flow-services/Frontend/#development","title":"Development","text":"<p>Check the official guide.</p>"},{"location":"Visual-Flow-services/Frontend/#contribution","title":"Contribution","text":"<p>Check the official guide.</p>"},{"location":"Visual-Flow-services/Frontend/#license","title":"License","text":"<p>Visual Flow is an open-source software licensed under the Apache-2.0 license.</p>"},{"location":"Visual-Flow-services/Frontend/DEVELOPMENT/","title":"Development Setup","text":"<p>In order to run the application you need to provide next environment variables.</p>"},{"location":"Visual-Flow-services/Frontend/DEVELOPMENT/#node-proxy","title":"Node Proxy","text":"<ul> <li><code>EXPRESS_PORT</code> - Port. [Default]: <code>8888</code>.</li> <li><code>API_SERVER</code> - Gateway API URL. [Example]: <code>https://${backend-url}/${backend-sub-path}</code>.</li> <li><code>MOCK_SERVER</code> - Mock server URL. [Example]: <code>http://localhost:3010</code>.</li> <li><code>SESSION_SECRET</code> - Your session secret.</li> <li><code>SESSION_STORE</code> - Session store. [Default]: <code>dynamic</code>.</li> <li><code>dynamic</code> - The store will be based on the env the app is running in: <code>development/production</code> - redis store, <code>local</code> - in memory store.</li> <li><code>in-memory</code> - In memory store. </li> <li><code>BASE_URL</code> - Base URL. [Example]: <code>/${frontend-sub-path}/</code>.</li> <li><code>LOGOUT_URL</code> - Logout URL for your OAuth provider. [Default]: <code>None</code>.</li> <li><code>COOKIE_MAX_AGE</code> - Cookie max age. [Default]: <code>3600 * 1000 * 8</code> (8 hours).</li> <li><code>BUILD_PATH</code> -  Directory with a production build (ReactJS) of the app. [Default]: <code>'../../frontend/public'</code>.</li> </ul>"},{"location":"Visual-Flow-services/Frontend/DEVELOPMENT/#redis-session-store","title":"Redis session store","text":"<p>You can use Redis as a session store. It can be useful for scaling purposes.</p> <ul> <li><code>REDIS_PORT</code> - Redis port. [Default]: <code>6379</code>.</li> <li><code>REDIS_HOST</code> - Redis host.</li> <li><code>REDIS_PASSWORD</code> - Redis password.</li> <li><code>REDIS_DB</code> - Database. [Default]: <code>0</code>.</li> </ul>"},{"location":"Visual-Flow-services/Frontend/DEVELOPMENT/#openid-connect-oidc","title":"OpenID Connect (OIDC)","text":"<ul> <li><code>STRATEGY</code> - OIDC.</li> <li><code>ISSUER_URL</code> - Issuer URL.</li> <li><code>AUTHORIZATION_URL</code> - Authorization URL.</li> <li><code>TOKEN_URL</code> - Token URL.</li> <li><code>USERINFO_URL</code> - User info URL.</li> <li><code>CLIENT_ID</code> - Client ID.</li> <li><code>CLIENT_SECRET</code> - Client secret.</li> <li><code>CALLBACK_URL</code> - Callback URL. [Example]: <code>https://localhost:8888/${frontend-sub-path}/callback</code>.</li> <li><code>OIDC_AVATAR_URL</code> - Endpoint template for fetching user/avatar info. [GitHub Example]: <code>https://api.github.com/users/${USERNAME}</code>.</li> <li><code>${USERNAME}</code> - This template variable will be replaced with a username. [Example]: <code>https://API/userByName/${USERNAME}</code>.</li> <li><code>${EMAIL}</code> - This template variable will be replaced with an email. [Example]: <code>https://API/userByEmail/${EMAIL}</code>.</li> <li><code>${ID}</code> - This template variable will be replaced with a user ID. [Example]: <code>https://API/userByID/${ID}</code>.</li> <li><code>OIDC_AVATAR_KEY</code> - JSON path for avatar URL. [GitHub Example]: <code>avatar_url</code>.</li> </ul>"},{"location":"Visual-Flow-services/Frontend/DEVELOPMENT/#oauth-github","title":"OAuth GITHUB","text":"<ul> <li><code>STRATEGY</code> - GITHUB.</li> <li><code>GITHUB_APP_ID</code> - GitHub App ID.</li> <li><code>GITHUB_APP_SECRET</code> - GitHub App Secret.</li> <li><code>STRATEGY_CALLBACK_URL</code> - GitHub Callback URL. [Example]: <code>https://localhost:8888/${frontend-sub-path}/callback</code>.</li> <li><code>STRATEGY_BASE_URL</code> - https://github.com.</li> </ul>"},{"location":"Visual-Flow-services/Frontend/DEVELOPMENT/#oauth-gitlab","title":"OAuth GITLAB","text":"<ul> <li><code>STRATEGY</code> - GITLAB.</li> <li><code>GITLAB_APP_ID</code> - GitLab App ID.</li> <li><code>GITLAB_APP_SECRET</code> - GitLab App Secret.</li> <li><code>STRATEGY_CALLBACK_URL</code> - GitLab Callback URL. [Example]: <code>https://localhost:8888/${frontend-sub-path}/callback</code>.</li> <li><code>STRATEGY_BASE_URL</code> - https://gitlab.com.</li> </ul> <p>Notes: - You can create <code>.env</code> file in backend root with all necessary values. - See documentation about configuring authentication at this link: https://github.com/ibagroup-eu/Visual-Flow-deploy/blob/main/OAUTH.md</p>"},{"location":"Visual-Flow-services/Frontend/DEVELOPMENT/#frontend-settings","title":"Frontend settings:","text":"<p>N/A</p>"},{"location":"Visual-Flow-services/Frontend/DEVELOPMENT/#start-locally-for-development","title":"Start locally for development","text":""},{"location":"Visual-Flow-services/Frontend/DEVELOPMENT/#frontend","title":"Frontend","text":"<ol> <li>Open frontend project folder <code>cd frontend</code>.</li> <li>Setup dependencies: <code>npm install</code> / <code>npm ci</code>.</li> <li>Run the app: <code>npm start</code>. It will be watching sources for change and rebuild.</li> </ol>"},{"location":"Visual-Flow-services/Frontend/DEVELOPMENT/#backend-node-proxy","title":"Backend (Node proxy)","text":"<ol> <li>Open backend project folder <code>cd backend</code>.</li> <li>Setup dependencies: <code>npm install</code> / <code>npm ci</code>.</li> <li>Set necessary environment variables (at least <code>MOCK_SERVER</code> and <code>API_SERVER</code>, <code>NODE_ENV=development</code>), f.e. in <code>backend/.env</code> file</li> <li>Run server in development mode: <code>npm start</code>. It will be watching sources for change and rebuild.</li> </ol>"},{"location":"Visual-Flow-services/Frontend/DEVELOPMENT/#json-server","title":"JSON-Server","text":"<ol> <li>Open backend project folder <code>cd json-server</code>.</li> <li>Setup dependencies: <code>npm install</code> / <code>npm ci</code>.</li> <li>Run server: <code>npm start</code>. It will be watching sources for change and rebuild.</li> </ol>"},{"location":"Visual-Flow-services/Frontend/DEVELOPMENT/#start-for-production","title":"Start for production","text":""},{"location":"Visual-Flow-services/Frontend/DEVELOPMENT/#frontend_1","title":"Frontend","text":"<ol> <li>Open frontend project folder <code>cd frontend</code>.</li> <li>Setup dependencies: <code>npm install</code> / <code>npm ci</code>.</li> <li>Build the app: <code>npm run build</code>.</li> </ol>"},{"location":"Visual-Flow-services/Frontend/DEVELOPMENT/#backend","title":"Backend","text":"<ol> <li>Open backend project folder <code>cd backend</code>.</li> <li>Setup dependencies: <code>npm install</code> / <code>npm ci</code>.</li> <li>Set necessary environment variables (at least <code>API_SERVER</code>, <code>NODE_ENV=production</code>),</li> <li>Run server in production mode: <code>npm run start:prod</code>.</li> </ol>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/","title":"About Visual Flow","text":"<p>Visual Flow is an ETL/ELT tool designed for effective data management via convenient and user-friendly interface. The tool has the following capabilities:</p> <ul> <li>Can integrate data from heterogeneous sources:</li> <li>Azure Blob Storage</li> <li>AWS S3</li> <li>Cassandra</li> <li>Click House</li> <li>DB2</li> <li>Databricks JDBC (global configuration)</li> <li>Databricks (Databricks configuration)</li> <li>Dataframe (for reading)</li> <li>Google Cloud Storage</li> <li>Elastic Search</li> <li>IBM COS</li> <li>Kafka</li> <li>Local File</li> <li>MS SQL</li> <li>Mongo</li> <li>MySQL/Maria</li> <li>Oracle</li> <li>PostgreSQL</li> <li>Redis</li> <li>Redshift</li> <li>REST API</li> <li>It supports the following file formats:</li> <li>Delta Lake</li> <li>Parquet</li> <li>JSON</li> <li>CSV</li> <li>ORC</li> <li>Avro</li> <li>Text</li> <li>Binary (PDF, DOC, Audio files)</li> <li>Leverage direct connectivity to enterprise applications as sources and targets</li> <li>Perform data processing and transformation</li> <li>Run custom code</li> <li>Leverage metadata for analysis and maintenance</li> <li>Allows to deploy in two configurations and run jobs in Spark/Kubernetes and Databricks environments respectively</li> <li>Leverages Generative AI capabilities via tasks like Parse text, Generate data, Transcribe, Generic task</li> </ul> <p>Visual Flow application is divided into the following repositories: </p> <ul> <li>Visual-Flow-frontend</li> <li>Visual-Flow-backend</li> <li>Visual-Flow-jobs (current)</li> <li>Visual-Flow-deploy</li> <li>Visual-Flow-backend-db-service</li> <li>Visual-Flow-backend-history-service</li> </ul>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/#visual-flow-jobs-repository","title":"Visual Flow Jobs Repository","text":"<p>This repository contains two types of jobs:</p> <ul> <li>Slack job - represents the notification stage in Visual Flow pipeline entity and is written in Python.</li> <li>Spark job - represents the Visual Flow job entity and is written in Scala.</li> </ul> <p>As Visual flow jobs consist of different stages, below is the list of stages that are supported by spark-job:</p> <ul> <li>Read stage.</li> <li>Write stage.</li> <li>Group By stage.</li> <li>Remove duplicates stage.</li> <li>Filter stage.</li> <li>Transformer stage.</li> <li>Join stage.</li> <li>Change data capture stage.</li> <li>Union stage.</li> <li>AI Text task stage.</li> </ul> <p>For more information on stages check stage_fields.md</p>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/#development","title":"Development","text":"<p>Check the official guide.</p>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/#contribution","title":"Contribution","text":"<p>Check the official guide</p>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/#license","title":"License","text":"<p>Visual flow is an open-source software licensed under the Apache-2.0 license.</p>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/DEVELOPMENT/","title":"Set up development environment","text":""},{"location":"Visual-Flow-services/Jobs%28Spark%29/DEVELOPMENT/#requirements","title":"Requirements","text":"<p>1) Make sure that you have the following software installed</p> Software Version Purpose Java 11 To supplement Scala which is used in spark jobs Scala 2.12.12 To work with spark jobs Maven 3+ To manage spark jobs' dependencies Python 3.8 To work with slack jobs pip should be built into Python, if not then install the latest version To manage slack jobs' dependencies Spark 3.4.1 To run spark jobs Kubernetes CLI(optional) actual for target cluster To be able to run spark and slack jobs in a production-like environment <p>2) If you want to run slack jobs(on any environment), make sure that you have these environment variables set</p> Name Explanation LOGGER_CONFIG_FILE_PATH Absolute path to a log file for the slack job SLACK_API_TOKEN Token to be able to work with Slack API <p>3) If you want to run spark jobs, make sure that you have these environment variables set</p> Name Explanation JOB_CONFIG Job configuration which consists of the stages. Check stage_fields.md for more information"},{"location":"Visual-Flow-services/Jobs%28Spark%29/DEVELOPMENT/#how-to-run","title":"How to run","text":""},{"location":"Visual-Flow-services/Jobs%28Spark%29/DEVELOPMENT/#slack-jobs","title":"Slack jobs","text":"<p>Regardless of the environment you want to run this in, it's pretty simple. - Make sure that you have python installed - Ensure that you have necessary env variables in place - Run the slack job via standard python command providing necessary CLI arguments</p> <p>If you want to run the slack job in kubernetes environment, check out Dockerfile</p>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/DEVELOPMENT/#spark-jobs","title":"Spark jobs","text":"<p>There are 3 most common ways of running spark jobs:</p>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/DEVELOPMENT/#1-in-kubernetes-via-backend-api","title":"1. In kubernetes via backend API","text":"<p>This requires you to have a properly configured and running backend API that has the connection to the Kubernetes server you want to run this on.</p> <p>In order to run the job you have to: - create/pick a Visual Flow job in backend API - run it through a specific endpoint</p> <p>The API takes care of Pod creation/configuration and will execute spark-submit for you via entrypoint.sh. </p> <p>Entrypoint script uses quite a lot of environment variables. You can find their description below.</p> Name Explanation JOB_MASTER URL to kubernetes cluster API DRIVER_MEMORY Amount of memory to use for the driver process DRIVER_CORES Number of cores to use for the driver process, only in cluster mode DRIVER_REQUEST_CORES Specify the cpu request for the driver pod EXECUTOR_MEMORY Amount of memory to use per executor process EXECUTOR_CORES The number of cores to use on each executor EXECUTOR_REQUEST_CORES Specify the cpu request for each executor pod EXECUTOR_INSTANCES Number of executors IMAGE_PULL_SECRETS Comma separated list of Kubernetes secrets used to pull images from private image registries SHUFFLE_PARTITIONS The default number of partitions to use when shuffling data for joins or aggregations POD_IP Hostname or IP address for the driver POD_NAME The name of your application POD_NAMESPACE The namespace that will be used for running the driver and executor pods JOB_IMAGE Container image to use for the Spark application JOB_ID Executor label for job's id PIPELINE_JOB_ID Executor label for stage's id JOB_JAR Path to spark job jar"},{"location":"Visual-Flow-services/Jobs%28Spark%29/DEVELOPMENT/#2-in-kubernetes-by-executing-spark-submit-locally","title":"2. In kubernetes by executing spark-submit locally","text":"<p>This requires you to have: - spark installed locally - Kubernetes CLI with proper .kube/config file - compiled spark jobs as a .jar file</p> <p>You'll have to manually create the pod and set up all necessary environment variables(especially the one that holds job configuration). Then you may use entrypoint.sh as the example of spark-submit command that you have to execute.</p>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/DEVELOPMENT/#3-executing-spark-submit-locally","title":"3. Executing spark-submit locally","text":"<p>This requires you to have: - spark installed locally - compiled spark jobs as a .jar file</p> <p>Make sure to set up all necessary environment variables(especially the one that holds job configuration).</p> <p>Then just execute spark-submit command with \"client\" deploy-mode and \"local\" master. You can find the example of command for local execution below:</p> <pre><code>spark-submit \\\n  --master \"local\" \\\n  --deploy-mode \"client\" \\\n  \"$JOB_JAR\"\n</code></pre> <p>$JOB_JAR is the path to spark-jobs .jar</p>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/","title":"Stage fields","text":""},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#spark-job-stages-parameters","title":"Spark Job Stages parameters","text":"<p>Stage is element of execution in Spark Job.  Execution order is defined by graph with nodes connected by edges. Each node is stage configured with its own set of input parameters, identified with unique id that provided in JSON format.</p>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#agreements","title":"Agreements","text":"<ul> <li>field names are case-sensitive string</li> <li>field values are case-sensitive strings</li> <li>fields are mandatory by default</li> </ul>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#this-document-agreements","title":"This document agreements","text":"<ul> <li>field values in angle brackets ('&lt;' and '&gt;') should be provided by user</li> <li>texts starting with '//' are comments and not present in actual requests</li> <li>column1(,column2(,..)) means one or more entries separated by comma, e.g. </li> <li>column1</li> <li>column1,column2</li> <li>column1,column2,column3</li> </ul>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#read-stages","title":"READ stages","text":""},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#read-generator","title":"Read generator","text":"<pre><code>{\n  \"id\": \"&lt;id&gt;\"\n  \"value\": {\n    \"operation\": \"READ\",\n    \"storage\": \"GENERATOR\",\n    \"alias\": \"&lt;alias&gt;\" // alias of resulting Spark DataFrame \n  }\n}\n</code></pre>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#read-jdbc","title":"Read JDBC","text":"<pre><code>{\n  \"id\": \"&lt;id&gt;\",\n  \"value\": {\n    \"operation\": \"READ\",\n    \"storage\": \"&lt;storage&gt;\", // supported values: 'db2', 'sqlite', 'oracle', 'mysql', 'postgresql', 'mssql', 'redshift-jdbc'\n    \"jdbcUrl\": \"jdbc:&lt;storage&gt;:&lt;parameters&gt;\",\n    \"schema\": \"&lt;database schema&gt;\",\n    \"table\": \"&lt;database table name&gt;\",\n    \"user\": \"&lt;credentials: user&gt;\",\n    \"password\": \"&lt;credentials: password&gt;\",\n    \"schema\": \"&lt;database schema&gt;\", // optional\n    \"certData\": \"&lt;db2 certificate&gt;\", // optional\n  }\n}\n</code></pre>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#read-object-storage","title":"Read object storage","text":"<pre><code>{\n  \"id\": \"&lt;id&gt;\",\n  \"value\": {\n    \"operation\": \"READ\",\n    \"storage\": \"&lt;storage&gt;\", // supported values: 'cos', 's3', 'azure-blob-storage', 'google-cloud-storage'\n    \"accessKey\": \"&lt;access key&gt;\", // credentials\n    \"secretKey\": \"&lt;secret key&gt;\", // credentials\n    \"bucket\": \"&lt;bucket&gt;\", // for 'cos', 's3', 'google-cloud-storage'\n    \"container\": \"&lt;container&gt;\", // for 'azure-blob-storage'\n    \"path\": \"&lt;path&gt;\", // path in bucket, for 'cos', 's3', 'google-cloud-storage'\n    \"containerPath\": \"&lt;path&gt;\", // path in container, for 'azure-blob-storage'\n    \"format\": \"&lt;format&gt;\", // Spark DataFrame format, e.g. 'csv', 'parquet', 'json', etc.\n    \"endpoint\": \"&lt;endpoint&gt;\" // only for 'cos' and 's3'\n  }\n}\n</code></pre>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#read-elastic","title":"Read elastic","text":"<pre><code>{\n  \"id\": \"&lt;id&gt;\",\n  \"value\": {\n    \"operation\": \"READ\",\n    \"storage\": \"elastic\",\n    \"nodes\": \"&lt;nodes&gt;\", // elasticsearch nodes to connect to (separated by comma)\n    \"port\": \"&lt;port&gt;\", // elasticsearch port (can be overriden in 'nodes' field)\n    \"ssl\": \"&lt;true or false&gt;\", // enables 'https' connections\n    \"certData\": \"&lt;elastic certificate&gt;\", // optional, use if 'ssl' enabled\n    \"user\": \"&lt;credentials: user&gt;\",\n    \"password\": \"&lt;credentials: password&gt;\",\n    \"index\": \"&lt;index&gt;\" // elasticsearch index (path to data in elastic db)\n  }\n}\n</code></pre>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#write-stages","title":"WRITE stages","text":""},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#write-to-console","title":"Write to console","text":"<pre><code>{\n  \"id\": \"&lt;id&gt;\",\n  \"value\": {\n    \"operation\": \"WRITE\",\n    \"storage\": \"STDOUT\"\n  }\n}\n</code></pre>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#write-jdbc","title":"Write JDBC","text":"<pre><code>{\n  \"id\": \"&lt;id&gt;\",\n  \"value\": {\n    \"operation\": \"WRITE\",\n    \"storage\": \"&lt;storage&gt;\",\n    \"jdbcUrl\": \"jdbc:&lt;storage&gt;:&lt;parameters&gt;\",\n    \"table\": \"&lt;database table name&gt;\",\n    \"user\": \"&lt;credentials: user&gt;\",\n    \"password\": \"&lt;credentials: password&gt;\",\n    \"schema\": \"&lt;database schema&gt;\", // optional\n    \"certData\": \"&lt;db2 certificate&gt;\", // optional\n    \"writeMode\": \"&lt;write mode&gt;\" // optional, see details below\n  }\n}\n</code></pre> <p>see writeMode values here (some values: errorifexists(default), overwrite, append, ignore)</p>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#write-object-storage","title":"Write object storage","text":"<pre><code>{\n  \"id\": \"&lt;id&gt;\",\n  \"value\": {\n    \"operation\": \"WRITE\",\n    \"storage\": \"&lt;storage&gt;\", // supported values: 'cos', 's3', 'azure-blob-storage', 'google-cloud-storage'\n    \"accessKey\": \"&lt;access key&gt;\", // credentials\n    \"secretKey\": \"&lt;secret key&gt;\", // credentials\n    \"bucket\": \"&lt;bucket&gt;\", // for 'cos', 's3', 'google-cloud-storage'\n    \"container\": \"&lt;container&gt;\", // for 'azure-blob-storage'\n    \"path\": \"&lt;path&gt;\", // path in bucket, for 'cos', 's3', 'google-cloud-storage'\n    \"containerPath\": \"&lt;path&gt;\", // path in container, for 'azure-blob-storage'\n    \"format\": \"&lt;format&gt;\", // Spark DataFrame format, e.g. 'csv', 'parquet', 'json', etc.\n    \"endpoint\": \"&lt;endpoint&gt;\", // only for 'cos' and 's3'\n    \"writeMode\": \"&lt;write mode&gt;\" // optional, see details in 'Write JDBC' section\n  }\n}\n</code></pre>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#write-elastic","title":"Write elastic","text":"<pre><code>{\n  \"id\": \"&lt;id&gt;\",\n  \"value\": {\n    \"operation\": \"WRITE\",\n    \"storage\": \"elastic\",\n    \"nodes\": \"&lt;nodes&gt;\", // elasticsearch nodes to connect to (separated by comma)\n    \"port\": \"&lt;port&gt;\", // elasticsearch port (can be overriden in 'nodes' field)\n    \"ssl\": \"&lt;true or false&gt;\", // enables 'https' connections\n    \"certData\": \"&lt;elastic certificate&gt;\", // optional, use if 'ssl' enabled\n    \"user\": \"&lt;credentials: user&gt;\",\n    \"password\": \"&lt;credentials: password&gt;\",\n    \"index\": \"&lt;index&gt;\" // elasticsearch index (path to data in elastic db),\n    \"writeMode\": \"&lt;write mode&gt;\" // optional, see details in 'Write JDBC' section    \n  }\n}\n</code></pre>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#readwrite-options","title":"Read/Write options","text":"<p>To pass configuration parameters to read and write stages (Object Storage and Elastic supported) option fields should be added in following format: <code>option.&lt;parameter name&gt;: &lt;parameter value&gt;</code> Example of cos csv output with changed delimiter and enabled header:  </p> <pre><code> {\n   \"id\": \"id_cos\",\n   \"value\": {\n     \"operation\": \"WRITE\",\n     \"storage\": \"cos\",\n     \"accessKey\": \"xxx\",\n     \"secretKey\": \"yyy\",\n     \"bucket\": \"bucket-name\",\n     \"path\": \"/path/to/file\",\n     \"format\": \"csv\",\n     \"option.delimiter\": \"@\",\n     \"option.header\": \"true\",\n     \"writeMode\": \"append\",\n     \"endpoint\": \"endpoint.example.com\"\n   }\n }\n</code></pre>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#function-stages","title":"FUNCTION stages","text":""},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#change-data-capture","title":"Change data capture","text":"<pre><code>{\n  \"id\": \"&lt;id&gt;\",\n  \"value\": {\n    \"operation\": \"CDC\",\n    \"keyColumns\": \"&lt;column1(,column2(,..))&gt;\" // key columns for CDC operation\n  }\n}\n</code></pre>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#filter","title":"Filter","text":"<pre><code>{\n  \"id\": \"&lt;id&gt;\",\n  \"value\": {\n    \"operation\": \"FILTER\",\n    \"condition\": \"&lt;where condition&gt;\" // part of SQL statement after 'where' keyword\n  }\n}\n</code></pre>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#group-by","title":"Group by","text":"<pre><code>{\n  \"id\": \"&lt;id&gt;\",\n  \"value\": {\n    \"operation\": \"GROUP\",\n    \"groupingColumns\": \"&lt;column1(,column2(,..))&gt;\", // SQL grouping columns\n    \"groupingCriteria\": \"opColumn1:operation1(,opColumn2:operation2(,..))\", // SQL aggregate columns and operations\n    \"dropGroupingColumns\": \"&lt;dropColumns&gt;\" // boolean\n  }\n}\n</code></pre>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#join","title":"Join","text":"<pre><code>{\n  \"id\": \"&lt;id&gt;\",\n  \"value\": {\n    \"operation\": \"JOIN\",\n    \"joinType\": \"&lt;type&gt;\", // SQL join type: e.g. 'inner', 'left', 'left outer', ...\n    \"columns\": \"&lt;column1(,column2(,..))&gt;\" // join columns\n  }\n}\n</code></pre>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#remove-duplicates","title":"Remove duplicates","text":"<pre><code>{\n  \"id\": \"&lt;id&gt;\",\n  \"value\": {\n    \"operation\": \"REMOVE_DUPLICATES\",\n    \"keyColumns\": \"&lt;column1(,column2(,..))&gt;\", // group columns: records considered duplicated if their values in 'column1', 'column2', ... are equal\n    \"orderColumns\": \"&lt;sortColumn1(,sortColumn2(,..))&gt;\" // sort columns: resulting record selected useing this sort, other records removed, to define sort following formats also supported: 'column:asc' and 'column:desc'\n  }\n}\n</code></pre>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#transform","title":"Transform","text":"<pre><code>{\n  \"id\": \"&lt;id&gt;\",\n  \"value\": {\n    \"operation\": \"TRANSFORM\",\n    \"statement\": \"&lt;SQL select statement&gt;\",  // part of SQL statement between 'select' and 'from' keywords\n  }\n}\n</code></pre> <p>see available Spark select functions here</p>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#union","title":"Union","text":"<pre><code>{\n  \"id\": \"&lt;id&gt;\",\n  \"value\": {\n    \"operation\": \"UNION\",\n    \"type\": \"&lt;type&gt;\" // supported values: 'distinct', 'all'\n  }\n}\n</code></pre>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#ai-text-task-parse-text","title":"AI Text Task - Parse text","text":"<pre><code>{\n  \"id\": \"&lt;id&gt;\",\n  \"value\": {\n    \"operation\": \"AI_TEXT_TASK\",\n    \"task\": \"parseText\",\n    \"llm\": \"&lt;LLM&gt;\", // 'ChatGPT'\n    \"endpoint\": \"&lt;endpoint&gt;\", // 'https://api.openai.com/v1/chat/completions'\n    \"model\": \"&lt;model&gt;\", // 'gpt-3.5-turbo'\n    \"apiKey\": \"&lt;api key&gt;\",\n    \"sourceColumn\": \"&lt;source column&gt;\",\n    \"maxTokens\": \"&lt;max tokens&gt;\", // e.g. 1024\n    \"temperature\": \"&lt;temperature&gt;\", // e.g. 0.7\n    \"systemMessage\": \"&lt;system message&gt;\", // e.g. 'You are client feedback assistant for the airline company'\n    \"keepExtraAttributes\": \"false\",\n    \"attributes\": \"&lt;attributes&gt;\", // attributes in JSON format encoded in base64\n    \"examples\": \"&lt;examples&gt;\" // examples in JSON format encoded in base64\n  }\n}\n</code></pre> <p>other tasks include: Generate Date - generate synthetic data, Transcribe - transcribe speech to text, Generic task - for the rest of use cases</p>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#cache","title":"Cache","text":"<pre><code>{\n  \"id\": \"&lt;id&gt;\",\n  \"value\": {\n    \"operation\": \"CACHE\",\n    \"useDisk\" : \"&lt;true or false&gt;\",\n    \"useMemory\" : \"&lt;true or false&gt;\",\n    \"useOffHeap\" : \"&lt;true or false&gt;\",\n    \"deserialized\" : \"&lt;true or false&gt;\",\n    \"replication\" : \"&lt;positive integer&gt;\"\n  }\n}\n</code></pre> <p>some recommended cache strategies:  </p> name \\ parameter value useDisk useMemory useOffHeap deserialized replication NONE false false false false 1 DISK_ONLY true false false false 1 DISK_ONLY_2 true false false false 2 MEMORY_ONLY false true false true 1 MEMORY_ONLY_2 false true false true 2 MEMORY_ONLY_SER false true false false 1 MEMORY_ONLY_SER_2 false true false false 2 MEMORY_AND_DISK true true false true 1 MEMORY_AND_DISK_2 true true false true 2 MEMORY_AND_DISK_SER true true false false 1 MEMORY_AND_DISK_SER_2 true true false false 2 OFF_HEAP true true true false 1"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#example","title":"Example","text":"<pre><code>{\n  \"nodes\": [\n    {\n      \"id\": \"gen-table1\",\n      \"value\": {\n        \"operation\": \"READ\",\n        \"storage\": \"GENERATOR\",\n        \"alias\": \"left\"\n      }\n    },\n    {\n      \"id\": \"dest-table1\",\n      \"value\": {\n        \"operation\": \"WRITE\",\n        \"storage\": \"sqlite\",\n        \"jdbcUrl\": \"jdbc:sqlite:src/main/resources/test.sqlite\",\n        \"table\": \"p1\",\n        \"user\": \"user\",\n        \"writeMode\": \"overwrite\",\n        \"password\": \"password\"\n      }\n    },\n    {\n      \"id\": \"source-table1\",\n      \"value\": {\n        \"operation\": \"READ\",\n        \"storage\": \"sqlite\",\n        \"jdbcUrl\": \"jdbc:sqlite:src/main/resources/test.sqlite\",\n        \"table\": \"p1\",\n        \"user\": \"user\",\n        \"password\": \"password\"\n      }\n    },\n    {\n      \"id\": \"print1\",\n      \"value\": {\n        \"operation\": \"WRITE\",\n        \"storage\": \"STDOUT\"\n      }\n    }\n\n  ],\n  \"edges\": [\n    {\n      \"source\": \"gen-table1\",\n      \"target\": \"dest-table1\"\n    },\n    {\n      \"source\": \"source-table1\",\n      \"target\": \"print1\"\n    }\n\n  ]\n}\n</code></pre>"},{"location":"Visual-Flow-services/Jobs%28Spark%29/stage_fields/#more-examples","title":"More examples","text":"<p>Check the examples</p>"}]}